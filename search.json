[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "noklam",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "noklam",
    "section": "Install",
    "text": "Install\npip install noklam"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "noklam",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Data Science & Software Engineering",
    "section": "",
    "text": "fsdl\n\n\n\n\nLecture & Lab notes\n\n\n\n\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\neuropython\n\n\n\n\nNotes for EuroPython2022 (Update daily)\n\n\n\n\n\n\nJul 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nI have been working with on some unit tests recently with mocking. There are some traps that I falled into and I want to document it here.\n\n\n\n\n\n\nMay 30, 2022\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nExploring the Python’s dataclass\n\n\n\n\n\n\nApr 22, 2022\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nSome unfinished notes about Python lower level details.\n\n\n\n\n\n\nFeb 10, 2022\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nproduct\n\n\n\n\nThe Long Beach port congestion could teach us a lot about data science in real world.\n\n\n\n\n\n\nNov 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nToday I encountered an interesting bug that I think it is worth to write it down for my future self.\n\n\n\n\n\n\nAug 18, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ndesign pattern\n\n\nsoftware\n\n\n\n\nA mini collections of design pattern for Data Science - Starting with callbacks.\n\n\n\n\n\n\nJul 10, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nreviewnb\n\n\nsql\n\n\n\n\nMaking collboration with Notebook possible and share perfect SQL analysis with Notebook.\n\n\n\n\n\n\nJun 26, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nlogging.config.dictConfig()\n\n\n\n\n\n\nJun 20, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython-internal\n\n\n\n\nDespite the bad reputation of GIL, it was arguably designed on purpose. The GIL actually comes with a lot of benefit.\n\n\n\n\n\n\nMay 29, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes - This lecture is about Recurrent Neural Network. Key concetps included input gate, forget gate, cell state, and output gate. It also explains how attention mechanism works for a encoder-decoder based architecture.\n\n\n\n\n\n\nApr 16, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nazure\n\n\n\n\nThis note helps you to prepare the Azure Assoicate Data Scientist DP-100 exam. I took DP100 in Mar 2021 and includes some important notes for study. Particularly, syntax types questions are very common. You need to study the lab and make sure you understand and remember some syntax to pass this exam.\n\n\n\n\n\n\nMar 27, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfsdl\n\n\n\n\nLecture & Lab notes, explain DataModules, Trainer LightningModule.\n\n\n\n\n\n\nMar 21, 2021\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\npickle\n\n\ndeepcopy\n\n\n\n\nAt first sight, these 3 things may not sounds related at all. I am writing this article to share a bug with lightgbm that I encountered and it eventually leads to deeper understanding of what pickle really are.\n\n\n\n\n\n\nMar 19, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nUnit Test with Pytest. I want to display a full information rich string into my CI Log, but it is trimmed by Python. Here is a simple fix to make it display full string in the log file.\n\n\n\n\n\n\nMar 17, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npyodbc\n\n\nimpala\n\n\n\n\nEasiest way to connect with Impala in Windows\n\n\n\n\n\n\nMar 5, 2021\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ncli\n\n\ntyper\n\n\n\n\nCreating python command line could be useful for a lot of tools. Traditionally, argparse has been used heavily, a new library called typer leverage python type hint, that makes creating command line interface in Python much pleasant.\n\n\n\n\n\n\nDec 10, 2020\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nKedro\n\n\n\n\n\n\nDec 4, 2020\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\n\n\nInspired by https://www.reddit.com/r/dataisbeautiful/comments/bjp8bg/the_united_states_of_elevation_oc/. This is my little weekend project, Hong Kong elevation tile with rayshader, powered by fastpages with Jupyter notebook! I haven’t used R in years, so I spent a lot more time than expected to finish this.\n\n\n\n\n\n\nNov 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nHave you ever deal with data files that does not fit into your memory? Here is a function that just trim memory footprint for you. This post is base on https://www.dataquest.io/blog/pandas-big-data/ and updated with a new automated functions from https://github.com/ianozsvald/dtype_diet/blob/master/dtype_diet.py\n\n\n\n\n\n\nNov 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nDo you know that you can have ggplot in BBC R graphics cookbook? This is an attempt to reproduce https://bbc.github.io/rcookbook/ in python\n\n\n\n\n\n\nApr 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nAre you tired with fiddling with every single python plot and you always forgot how to configure matplotlib? Start creating your own matplotlib style or just use this. [Updated on 17-07-2022]\n\n\n\n\n\n\nApr 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nkaggle\n\n\n\n\nMy first silver medal on Kaggles Competition - Bengali Image Classification Competition. 10 lessons learn!\n\n\n\n\n\n\nMar 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nfastai\n\n\n\n\nDispatch is an amazing useful features which is underused in Python. In this article, I will show you how you can use it to make Python more powerful.\n\n\n\n\n\n\nFeb 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nA tutorial of fastpages for Jupyter notebooks.\n\n\n\n\n\n\nFeb 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nML\n\n\n\n\nMixup and Beta Distribution\n\n\n\n\n\n\nFeb 9, 2020\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nnoklam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nDesktop notifiaction with Python\n\n\n\n\n\n\nOct 19, 2019\n\n\nnoklam\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ncodespaces\n\n\nnbdev\n\n\n\n\nHow a new GitHub feature makes literate programming easier than ever before.\n\n\n\n\n\n\nJan 1, 2019\n\n\nHamel Husain & Jeremy Howard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "href": "blog/posts/2021-06-20-logging-config-dict-issue-kedro.html",
    "title": "A logging.config.dictConfig() issue in python",
    "section": "",
    "text": "With this code block, you will find no print() or logging is sent to ClearML logging Console. Turns out kedro use logging.config.dictConfig(conf_logging) as the default and causing this issue.\nA quick fix is to add \"incremental\": True in the config dict. In the standard documentation, the default is False, which means the configuration will replace existing one, thus removing the clearml handlers, and causing the issue I had.\nconf_logging = {\"version\":1, \n                \"incremental\": True\n                \"formatters\":{\n                      \"simple\":{\n                             \"format\":\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                      }\n                  }"
  },
  {
    "objectID": "blog/posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "href": "blog/posts/2022-02-10-journey-of-understanding-python-and-programming-langauge.html",
    "title": "Journey of understanding Python and programming language",
    "section": "",
    "text": "What is Bytecode?\n\n\nPython Virtual Machine\n\n\nCompiler\n\n\nEBNF Grammar\n\n\nLLVM"
  },
  {
    "objectID": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "href": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/mixup-beta"
  },
  {
    "objectID": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "href": "blog/posts/2020-02-09-MixUp-and-Beta-Distribution.html#beta-distribution",
    "title": "data augmentation - Understand MixUp and Beta Distribution",
    "section": "Beta Distribution",
    "text": "Beta Distribution\nBeta distribution is control by two parameters, α and β with interval [0, 1], which make it useful for Mixup. Mixup is basically a superposition of two image with a parameter t. Instead of using a dog image, with Mixup, you may end up have a image which is 0.7 dog + 0.3 cat\nTo get some sense of what a beta distribution is, let plot beta distribution with different alpha and beta to see its effect\nimport math\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import tensor\n# PyTorch has a log-gamma but not a gamma, so we'll create one\nΓ = lambda x: x.lgamma().exp()\nfacts = [math.factorial(i) for i in range(7)]\n\nplt.plot(range(7), facts, 'ro')\nplt.plot(torch.linspace(0,6), Γ(torch.linspace(0,6)+1))\nplt.legend(['factorial','Γ']);\n\n\n\npng"
  },
  {
    "objectID": "blog/posts/2022-07-11-europython2022-summary.html",
    "href": "blog/posts/2022-07-11-europython2022-summary.html",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "",
    "text": "EuroPython2022\nSchedule: https://ep2022.europython.eu/schedule/\nSession that I attended: #europython"
  },
  {
    "objectID": "blog/posts/2022-07-11-europython2022-summary.html#bulletproof-python-property-based-testing-with-hypothesis",
    "href": "blog/posts/2022-07-11-europython2022-summary.html#bulletproof-python-property-based-testing-with-hypothesis",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "Bulletproof Python – Property-Based Testing with Hypothesis",
    "text": "Bulletproof Python – Property-Based Testing with Hypothesis\nThe term property based testing isn’t too important. In a nutshell hypothesis is a python library that help you to write (better) tests by modifying your workflow.\n\nPrepare mock data Provide a specification of data, let hypothesis do the work\nPerform some operation\nAssert the result with expected value\n\nThe rationale behind this is\n\n\n\n\n\n\nNote\n\n\n\n** People write code don’t come up with good test. **\n\n\nFor example, you can generate integers with hypotesis.strategies.integers, it does something smart under the hood so it’s not just random number but more meaningful test. For example, you usually want to test for zero, negative number, positive number, large number. hypoethsis try to maximize the variety of tests and you just need to give it a specification.\nYou can also generate more sophisticated data, for example, a tuple of two integers, where the second integer has to be larger than the first one.\n@st.composite\n\ndef list_and_index(draw, elements=st.integers()):\n    first = draw(elements)\n    second = draw(st.integers(min_value=first + 1))\n    return (first, second)\nThink of it as your virtual QA buddy."
  },
  {
    "objectID": "blog/posts/2022-07-11-europython2022-summary.html#tdd-development-with-pytest",
    "href": "blog/posts/2022-07-11-europython2022-summary.html#tdd-development-with-pytest",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "TDD Development with pytest",
    "text": "TDD Development with pytest\nWorkflow for TDD 1. Pick one bug/missing feature 2. Write a test that fails 3. Minimal amount of code that pass - (even hard coded!) 4. Refactor\nThere are good questions asked * In case of you don’t know what’s the expected answer, how do you write test that fails meaningfully?\nI jump out of the session because of a call, so not too many comments about this session. In general I like the idea of TDD but struggle to apply the textbook version of TDD as examples are often much simpler than the real application.\nFew key points * Tests as specification about your program (What it does and what not) * Understand why you test fail and pass. * Tests are also good source of documentation.\nThinking about test first also force you to think more about the design, you almost start from pseudocode (you function doesn’t even exist!)."
  },
  {
    "objectID": "blog/posts/2022-07-11-europython2022-summary.html#introduction-to-apache-tvm",
    "href": "blog/posts/2022-07-11-europython2022-summary.html#introduction-to-apache-tvm",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "Introduction to Apache TVM",
    "text": "Introduction to Apache TVM\n\n\n\nApache TVM\n\n\n\nApache TVM is a framework that try to squeeze extra performance from specialized hardware.\n\nIn practice, the workflow roughly go like this 1. Trained a model with your favored libraries (PyTorch/Tensorflow etc) 2. Use TVM to compile and tune -> After this you get a compiled module as output 3. Use TVM python API for inference\nThe performance gains are mainly from hardware architecture that can give better performance, TVM did some architecture search and try to find the optimal one.\n\n\n\n\n\n\nNote\n\n\n\nMaybe one side benefit of this is it does not need the deep learning pipeline dependecenies since you just need the TVM Python API and the model file for inference."
  },
  {
    "objectID": "blog/posts/2022-07-11-europython2022-summary.html#how-many-modules-imported-in-python-by-default",
    "href": "blog/posts/2022-07-11-europython2022-summary.html#how-many-modules-imported-in-python-by-default",
    "title": "EuroPython 2022 - Conference Notes & Summary",
    "section": "How many modules imported in Python by default?",
    "text": "How many modules imported in Python by default?\n\nPython Shell - 79\nIPython - 646!\nJupyter - 1020!!!\n\nIt’s quite surprising how many libraries are imported by default, and this explains why it takes some time whenever you do ipython on a shell, as the Python Interpreter is busy reading all the files and evalute it.\nSome other interesting notes: * Python use a Finder and Loader to import modules * sys.path is the order that Python Interpreter search for modules, and the first match wins (This is important if you have duplicate namespace or if you do namespace package) * Don’t do sys.path.append although you will find this very common if you do a Stackoverflow search, use environment variable PYTHONPATH=some_path instead"
  },
  {
    "objectID": "blog/posts/2019-10-19-Deskto-Notification.html",
    "href": "blog/posts/2019-10-19-Deskto-Notification.html",
    "title": "plyer - Desktop Notification with Python",
    "section": "",
    "text": "You could add this simple code block to notify you when the program is done! A desktop notification will be prompt on the bottom right corner in Window."
  },
  {
    "objectID": "blog/posts/2021-03-17-pytest-data-test-truncated-error.html",
    "href": "blog/posts/2021-03-17-pytest-data-test-truncated-error.html",
    "title": "Data Test as CI",
    "section": "",
    "text": "I created a custom error class that would do the job, however, pytest truncated my AssertionError since it is quite long.\nI am using pytest magic from https://github.com/akaihola/ipython_pytest which allow me to run pytest in a Jupyter notebook cell.\nIt is quite simple with a few tens of lines.\n\nimport os\nimport shlex\nimport sys\nfrom pathlib import Path\n\nimport tempfile\nfrom IPython.core import magic\nfrom pytest import main as pytest_main\n\n\nTEST_MODULE_NAME = '_ipytesttmp'\n\ndef pytest(line, cell):\n    with tempfile.TemporaryDirectory() as root:\n        oldcwd = os.getcwd()\n        os.chdir(root)\n        tests_module_path = '{}.py'.format(TEST_MODULE_NAME)\n        try:\n            Path(tests_module_path).write_text(cell)\n            args = shlex.split(line)\n            os.environ['COLUMNS'] = '80'\n            pytest_main(args + [tests_module_path])\n            if TEST_MODULE_NAME in sys.modules:\n                del sys.modules[TEST_MODULE_NAME]\n        finally:\n            os.chdir(oldcwd)\n\ndef load_ipython_extension(ipython):\n    magic.register_cell_magic(pytest)\n\nWriting ipython_pytest.py\n\n\n\n# !pip install pytest\n\nThe ipython_pytest extension is already loaded. To reload it, use:\n  %reload_ext ipython_pytest\n\n\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpohw9e_9w\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n>       assert x == expect\nE       AssertionError: assert 'placeholder' == 'abcdefg\\nabc...fg\\nabcdefg\\n'\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg...\nE         \nE         ...Full output truncated (15 lines hidden), use '-vv' to show\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s ==============================\n\n\nYou can see that pytest truncated my error with ... Here is how I solve ths issue\n\ndef test_long_assertion_error():\n    x = \"placeholder\"\n    expect = \"abcdefg\\n\"*20 # Long string\n    assert x == expect\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.3, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- c:\\programdata\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\channo\\AppData\\Local\\Temp\\tmpyic4vcra\ncollecting ... collected 1 item\n\n_ipytesttmp.py::test_long_assertion_error FAILED                         [100%]\n\n================================== FAILURES ===================================\n__________________________ test_long_assertion_error __________________________\n\n    def test_long_assertion_error():\n        x = \"placeholder\"\n        expect = \"abcdefg\\n\"*20 # Long string\n>       assert x == expect\nE       AssertionError: assert 'placeholder' == ('abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n'\\n 'abcdefg\\n')\nE         + placeholder\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\nE         - abcdefg\n\n_ipytesttmp.py:5: AssertionError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_long_assertion_error - AssertionError: assert 'pl...\n============================== 1 failed in 0.06s =============================="
  },
  {
    "objectID": "blog/posts/2020-02-20-test.html",
    "href": "blog/posts/2020-02-20-test.html",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "This notebook is a demonstration of some of capabilities of fastpages with notebooks.\nWith fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts!\n\n\nThe first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# Title\n> Awesome summary\n\n- toc: true- branch: master- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nMore details and options for front matter can be viewed on the front matter section of the README.\n\n\n\nA #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n\nCode\nimport pandas as pd\nimport altair as alt\n\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n\nCode\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\n\n\n\n\nCharts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\n\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(movies).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\n\n\nalt.Chart(movies).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y=alt.Y('IMDB_Rating:Q', axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=600,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=700,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\nYou can display tables per the usual way in your blog:\n\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\ndf = pd.read_json(movies)\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide_Gross\n      Production_Budget\n      Distributor\n      MPAA_Rating\n      IMDB_Rating\n      Rotten_Tomatoes_Rating\n    \n  \n  \n    \n      0\n      The Land Girls\n      146083.0\n      8000000.0\n      Gramercy\n      R\n      6.1\n      NaN\n    \n    \n      1\n      First Love, Last Rites\n      10876.0\n      300000.0\n      Strand\n      R\n      6.9\n      NaN\n    \n    \n      2\n      I Married a Strange Person\n      203134.0\n      250000.0\n      Lionsgate\n      None\n      6.8\n      NaN\n    \n    \n      3\n      Let's Talk About Sex\n      373615.0\n      300000.0\n      Fine Line\n      None\n      NaN\n      13.0\n    \n    \n      4\n      Slam\n      1087521.0\n      1000000.0\n      Trimark\n      R\n      3.4\n      62.0\n    \n  \n\n\n\n\n\n\n\n\n\nYou can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![123](my_icons/fastai_logo.png)\n\n\n\n\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg)\n\n\n\n\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\n\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "blog/posts/2020-02-20-test.html#tweetcards",
    "href": "blog/posts/2020-02-20-test.html#tweetcards",
    "title": "Fastpages Notebook Blog Post",
    "section": "Tweetcards",
    "text": "Tweetcards\nTyping > twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20"
  },
  {
    "objectID": "blog/posts/2020-02-20-test.html#youtube-videos",
    "href": "blog/posts/2020-02-20-test.html#youtube-videos",
    "title": "Fastpages Notebook Blog Post",
    "section": "Youtube Videos",
    "text": "Youtube Videos\nTyping > youtube: https://youtu.be/XfoYk_Z5AkI will render this:"
  },
  {
    "objectID": "blog/posts/2020-02-20-test.html#boxes-callouts",
    "href": "blog/posts/2020-02-20-test.html#boxes-callouts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Boxes / Callouts",
    "text": "Boxes / Callouts\nTyping > Warning: There will be no second warning! will render this:\n\n\n\n\n\n\nWarning\n\n\n\nThere will be no second warning!\n\n\nTyping > Important: Pay attention! It's important. will render this:\n\n\n\n\n\n\nImportant\n\n\n\nPay attention! It’s important.\n\n\nTyping > Tip: This is my tip. will render this:\n\n\n\n\n\n\nTip\n\n\n\nThis is my tip.\n\n\nTyping > Note: Take note of this. will render this:\n\n\n\n\n\n\nNote\n\n\n\nTake note of this.\n\n\nTyping > Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs:\n\n\n\n\n\n\nNote\n\n\n\nA doc link to an example website: fast.ai should also work fine."
  },
  {
    "objectID": "blog/posts/2020-02-20-test.html#footnotes",
    "href": "blog/posts/2020-02-20-test.html#footnotes",
    "title": "Fastpages Notebook Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\nYou can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this:\n{% raw %}For example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ 'This is the footnote.' | fndetail: 1 }}\n{{ 'This is the other footnote. You can even have a [link](www.github.com)!' | fndetail: 2 }}{% endraw %}\nFor example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ ‘This is the footnote.’ | fndetail: 1 }} {{ ‘This is the other footnote. You can even have a link!’ | fndetail: 2 }}"
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "",
    "text": "If you have ever written SQL queries to extract data from a database, chances are you are familiar with an IDE like the screenshot below. The IDE offers features like auto-completion, visualize the query output, display the table schema and the ER diagram. Whenever you need to write a query, this is your go-to tool. However, you may want to add Jupyter Notebook into your toolkit. It improves my productivity by complementing some missing features in IDE."
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-self-contained-report",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a self-contained report",
    "text": "Notebook as a self-contained report\nAs a data scientist/data analyst, you write SQL queries for ad-hoc analyses all the time. After getting the right data, you make nice-looking charts and put them in a PowerPoint and you are ready to present your findings. Unlike a well-defined ETL job, you are exploring the data and testing your hypotheses all the time. You make assumptions, which is often wrong but you only realized it after a few weeks. But all you got is a CSV that you cannot recall how it was generated in the first place.\nData is not stationary, why should your analysis be? I have seen many screenshots, fragmented scripts flying around in organizations. As a data scientist, I learned that you need to be cautious about what you heard. Don’t trust peoples’ words easily, verify the result! To achieve that, we need to know exactly how the data was extracted, what kind of assumptions have been made? Unfortunately, this information usually is not available. As a result, people are redoing the same analysis over and over. You will be surprised that this is very common in organizations. In fact, numbers often do not align because every department has its own definition for a given metric. It is not shared among the organization, and verbal communication is inaccurate and error-prone. It would be really nice if anyone in the organization can reproduce the same result with just a single click. Jupyter Notebook can achieve that reproducibility and keep your entire analysis (documentation, data, and code) in the same place."
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-an-extension-of-ide",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as an extension of IDE",
    "text": "Notebook as an extension of IDE\nWriting SQL queries in a notebook gives you extra flexibility of a full programming language alongside SQL. For example:\n\nWrite complex processing logic that is not easy in pure SQL\nCreate visualizations directly from SQL results without exporting to an intermediate CSV\n\nFor instance, you can pipe your SQL query with pandas and then make a plot. It allows you to generate analysis with richer content. If you find bugs in your code, you can modify the code and re-run the analysis. This reduces the hustles to reproduce an analysis greatly. In contrast, if your analysis is reading data from an anonymous exported CSV, it is almost guaranteed that the definition of the data will be lost. No one will be able to reproduce the dataset.\nYou can make use of the ipython_sql library to make queries in a notebook. To do this, you need to use the magic function with the inline magic % or cell magic %%.\n\nsales = %sql SELECT * from sales LIMIT 3\nsales\n\n\n\n    \n        ProductId\n        Unit\n        IsDeleted\n    \n    \n        1\n        10\n        1\n    \n    \n        1\n        10\n        1\n    \n    \n        2\n        10\n        0\n    \n\n\n\nTo make it fancier, you can even parameterize your query with variables. Tools like papermill allows you to parameterize your notebook. If you execute the notebook regularly with a scheduler, you can get a updated dashboard. To reference the python variable, the $ sign is used.\n\ntable = \"sales\"\nquery = f\"SELECT * from {table} LIMIT 3\"\nsales = %sql $query\nsales\n\n\n\n    \n        ProductId\n        Unit\n        IsDeleted\n    \n    \n        1\n        10\n        1\n    \n    \n        1\n        10\n        1\n    \n    \n        2\n        10\n        0\n    \n\n\n\nWith a little bit of python code, you can make a nice plot to summarize your finding. You can even make an interactive plot if you want. This is a very powerful way to extend your analysis.\n\nimport seaborn as sns\nsales = %sql SELECT * FROM SALES\nsales_df = sales.DataFrame()\nsales_df = sales_df.groupby('ProductId', as_index=False).sum()\nax = sns.barplot(x='ProductId', y='Unit', data=sales_df)\nax.set_title('Sales by ProductId');"
  },
  {
    "objectID": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "href": "blog/posts/2021-06-26-jupyter-sql-notebook.html#notebook-as-a-collaboration-tool",
    "title": "Jupyter Superpower - Extend SQL analysis with Python",
    "section": "Notebook as a collaboration tool",
    "text": "Notebook as a collaboration tool\nJupyter Notebook is flexible and it fits extremely well with exploratory data analysis. To share to a non-coder, you can share the notebook or export it as an HTML file. They can read the report or any cached executed result. If they need to verify the data or add some extra plots, they can do it easily themselves.\nIt is true that Jupyter Notebook has an infamous reputation. It is not friendly to version control, it’s hard to collaborate with notebooks. Luckily, there are efforts that make collaboration in notebook a lot easier now.\nHere what I did not show you is that the table has an isDeleted column. Some of the records are invalid and we should exclude them. In reality, this happens frequently when you are dealing with hundreds of tables that you are not familiar with. These tables are made for applications, transactions, and they do not have analytic in mind. Data Analytic is usually an afterthought. Therefore, you need to consult the SME or the maintainer of that tables. It takes many iterations to get the correct data that can be used to produce useful insight.\nWith ReviewNB, you can publish your result and invite some domain expert to review your analysis. This is where notebook shine, this kind of workflow is not possible with just the SQL script or a screenshot of your finding. The notebook itself is a useful documentation and collaboration tool.\n\nStep 1 - Review PR online\n\n\n\n\n\nStep1\n\n\nYou can view your notebook and add comments on a particular cell on ReviewNB. This lowers the technical barrier as your analysts do not have to understand Git. He can review changes and make comments on the web without the need to pull code at all. As soon as your analyst makes a suggestion, you can make changes.\n\n\nStep 2 - Review Changes\n\n\n\n\n\nStep2\n\n\nOnce you have made changes to the notebook, you can review it side by side. This is very trivial to do it in your local machine. Without ReviewNB, you have to pull both notebooks separately. As Git tracks line-level changes, you can’t really read the changes as it consists of a lot of confusing noise. It would also be impossible to view changes about the chart with git.\n\n\nStep 3 - Resolve Discussion\n\n\n\n\n\nStep3\n\n\nOnce the changes are reviewed, you can resolve the discussion and share your insight with the team. You can publish the notebook to internal sharing platform like knowledge-repo to organize the analysis.\nI hope this convince you that Notebook is a good choice for adhoc analytics. It is possible to collaborate with notebook with proper software in place. Regarless if you use notebook or not, you should try your best to document the process. Let’s make more reproducible analyses!"
  },
  {
    "objectID": "blog/posts/2022-05-30-mocking-with-pytest-patch.html#mock",
    "href": "blog/posts/2022-05-30-mocking-with-pytest-patch.html#mock",
    "title": "Testing with Mocking",
    "section": "Mock",
    "text": "Mock\n\nmock = Mock()\n\nWith the Mock object, you can treat it like a magic object that have any attributes or methods.\n\nmock.super_method(), mock.attribute_that_does_not_exist_at_all\n\n(<Mock name='mock.super_method()' id='1587554283232'>,\n <Mock name='mock.attribute_that_does_not_exist_at_all' id='1587554282512'>)\n\n\n\nstr(mock)\n\n\"<Mock id='1587554282848'>\""
  },
  {
    "objectID": "blog/posts/2022-05-30-mocking-with-pytest-patch.html#magicmock",
    "href": "blog/posts/2022-05-30-mocking-with-pytest-patch.html#magicmock",
    "title": "Testing with Mocking",
    "section": "MagicMock",
    "text": "MagicMock\nThe “magic” comes from the magic methods of python object, for example, when you add two object together, it is calling the __add__ magic method under the hook.\n\nmock + mock\n\nTypeError: unsupported operand type(s) for +: 'Mock' and 'Mock'\n\n\n\nmagic_mock = MagicMock()\n\n\nmagic_mock + magic_mock\n\n<MagicMock name='mock.__add__()' id='1587563722784'>\n\n\nWith MagicMock, you get these magic methods for free, this is why adding two mock will not throw an error but adding two Mock will result in a TypeError\nLet say we want to mock the pandas.read_csv function, because we don’t actually want it to read a data, but just return some mock data whenever it is called. It’s easier to explain with an example."
  },
  {
    "objectID": "blog/posts/2022-05-30-mocking-with-pytest-patch.html#mocker.patch-with-createtrue",
    "href": "blog/posts/2022-05-30-mocking-with-pytest-patch.html#mocker.patch-with-createtrue",
    "title": "Testing with Mocking",
    "section": "mocker.patch with create=True",
    "text": "mocker.patch with create=True\n\nimport pandas as pd\n\ndef test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=False)\n    assert pd.read_special_csv(\"some_data\") == \"fake_data\"\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmpzbddlxxg\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollected 1 item\n\n_ipytesttmp.py F                                                         [100%]\n\n================================== FAILURES ===================================\n________________________________ test_read_csv ________________________________\n\nmocker = <pytest_mock.plugin.MockFixture object at 0x00000171B28B1820>\n\n    def test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n>       mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=False)\n\n_ipytesttmp.py:4: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n..\\..\\..\\..\\miniconda3\\lib\\site-packages\\pytest_mock\\plugin.py:193: in __call__\n    return self._start_patch(self.mock_module.patch, *args, **kwargs)\n..\\..\\..\\..\\miniconda3\\lib\\site-packages\\pytest_mock\\plugin.py:157: in _start_patch\n    mocked = p.start()\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1529: in start\n    result = self.__enter__()\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1393: in __enter__\n    original, local = self.get_original()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <unittest.mock._patch object at 0x00000171B28B10D0>\n\n    def get_original(self):\n        target = self.getter()\n        name = self.attribute\n    \n        original = DEFAULT\n        local = False\n    \n        try:\n            original = target.__dict__[name]\n        except (AttributeError, KeyError):\n            original = getattr(target, name, DEFAULT)\n        else:\n            local = True\n    \n        if name in _builtins and isinstance(target, ModuleType):\n            self.create = True\n    \n        if not self.create and original is DEFAULT:\n>           raise AttributeError(\n                \"%s does not have the attribute %r\" % (target, name)\n            )\nE           AttributeError: <module 'pandas' from 'c:\\\\users\\\\lrcno\\\\miniconda3\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'> does not have the attribute 'read_special_csv'\n\n..\\..\\..\\..\\miniconda3\\lib\\unittest\\mock.py:1366: AttributeError\n=========================== short test summary info ===========================\nFAILED _ipytesttmp.py::test_read_csv - AttributeError: <module 'pandas' from ...\n============================== 1 failed in 0.43s ==============================\n\n\nNow we fail the test because pandas.read_special_csv does not exist. However, with create=True you can make the test pass again. Normally you won’t want to do this, but it is an option that available.\n\nimport pandas as pd\n\ndef test_read_csv(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_special_csv\", return_value = \"fake_data\", create=True)\n    assert pd.read_special_csv(\"some_data\") == \"fake_data\"\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmphqbckliw\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollected 1 item\n\n_ipytesttmp.py .                                                         [100%]\n\n============================== 1 passed in 0.10s ==============================\n\n\nMore often, you would want your mock resemble your real object, which means it has the same attributes and method, but it should fails when the method being called isn’t valid. You may specify the return_value with the mock type\n\nimport pandas as pd\nfrom unittest.mock import Mock\nimport pytest\n\ndef test_read_csv_valid_method(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_csv\", return_value = Mock(pd.DataFrame))\n    df =  pd.read_csv(\"some_data\")\n    df.mean()  # A DataFrame method\n\ndef test_read_csv_invalid_method(mocker):  # mocker is a special pytest fixture, so even though we haven't define it here but pytest understands it.\n    mocker.patch(\"pandas.read_csv\", return_value = Mock(pd.DataFrame))\n    df =  pd.read_csv(\"some_data\")\n    with pytest.raises(Exception):\n        df.not_a_dataframe_method()\n\n============================= test session starts =============================\nplatform win32 -- Python 3.8.5, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- c:\\users\\lrcno\\miniconda3\\python.exe\ncachedir: .pytest_cache\nrootdir: C:\\Users\\lrcno\\AppData\\Local\\Temp\\tmpyfiqtkoy\nplugins: anyio-3.5.0, cov-3.0.0, mock-1.13.0\ncollecting ... collected 2 items\n\n_ipytesttmp.py::test_read_csv_valid_method PASSED                        [ 50%]\n_ipytesttmp.py::test_read_csv_invalid_method PASSED                      [100%]\n\n============================== 2 passed in 0.16s =============================="
  },
  {
    "objectID": "blog/posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "href": "blog/posts/2021-04-16-full-stack-deep-learning-lecture-03.html",
    "title": "Full Stack Deep Learning Notes - Lecture 03 - Recurrent Neural Network",
    "section": "",
    "text": "# Google Neurl Machine Translation (GNMT)\nIt more or less follow the attention mechanism described here.\nhttps://blog.floydhub.com/attention-mechanism/#luong-att-step6\n\n\n\nattention_gnmt\n\n\n1.If you take the dot product of 1 encoder vector (at t_i) and decoder, you get a scalar. (Alignment Score) (1,h) * (h,1) -> (1,1) 2. If encoder have 5 time_step, repeat the above steps -> You get a vector with length of 5 (A vector of Alignment Scores) (5,h) (h,1) -> (5,1) 3. Take softmax of the alignments scores -> (attention weights which sum to 1) (5,1) 4. Take dot product of encoders state with attention weights (h, 5)  (5, 1) -> (h, 1), where h stands for dimension of hidden state. The result is a “Context Vector”"
  },
  {
    "objectID": "blog/posts/2021-03-05-pyodbc-linux.html",
    "href": "blog/posts/2021-03-05-pyodbc-linux.html",
    "title": "Setting up pyodbc for Impala connection, works on both Linux and Window",
    "section": "",
    "text": "Setup\nFirst, you need to download the ODBC driver from Cloudera.\nThen you need to instsall the driver properly.\ndpkg -i docker/clouderaimpalaodbc_2.6.10.1010-2_amd64.deb\nAdd this file to the directory /etc/odbcinst.ini, if you already have add, append this to the file.\n# /etc/odbcinst.ini\n[ODBC Drivers]\nCloudera Impala ODBC Driver 32-bit=Installed\nCloudera Impala ODBC Driver 64-bit=Installed\n[Cloudera Impala ODBC Driver 32-bit]\nDescription=Cloudera Impala ODBC Driver (32-bit)\nDriver=/opt/cloudera/impalaodbc/lib/32/libclouderaimpalaodbc32.so\n[Cloudera Impala ODBC Driver 64-bit]\nDescription=Cloudera Impala ODBC Driver (64-bit)\nDriver=/opt/cloudera/impalaodbc/lib/64/libclouderaimpalaodbc64.so\nThen install some additional package.\napt-get update && apt-get -y install gnupg apt-transport-https\napt-get update && apt-get -y install libssl1.0.0 unixodbc unixodbc-dev \\\n&& ACCEPT_EULA=Y apt-get -y install msodbcsql17\napt-get install unixodbc-dev -y\nLast, pip install pyodbc and have fun.\nTo read a database table, you can simply do this.\nimport pyodbc\nimport pandas as pd\n\nconn = pyodbc.connect(f\"\"\"\nDriver=Cloudera ODBC Driver for Impala 64-bit; \nPWD=password;\nUID=username;\nDatabase=database\n\"\"\")\nThere are multiple way to connect, but I found using a connection string is the most straight forward solution that does not require any additional enviornment variable setup."
  },
  {
    "objectID": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html",
    "href": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "",
    "text": "from dataclasses import dataclass, field, astuple\nWith dataclass, you can set frozen=True to ensure immutablilty.\nMutating a frozen dataclass is not possible, but what if I need to compose some logic? __post_init__() method is how you can customize logic."
  },
  {
    "objectID": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#post_init-assignment",
    "href": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#post_init-assignment",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "post_init assignment",
    "text": "post_init assignment\n\n@dataclass\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        self.c = self.a + self.b\nfrozen = FrozenDataClass(1,2)\n\n\nfrozen.a, frozen.b, frozen.c\n\n(1, 2, 3)\n\n\nDo notice that I removed the frozen=True flag, see what happen if I put it back."
  },
  {
    "objectID": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#the-good-old-property",
    "href": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#the-good-old-property",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "The good old @property?",
    "text": "The good old @property?\n\n@dataclass\nclass PartialFrozenDataClass:\n    a: int # a should be frozen \n    b: int # Should be mutable\n    \n    @property\n    def b(self):\n        return self.b\n    \np = PartialFrozenDataClass(1,2)\n\nAttributeError: can't set attribute\n\n\nIt doesn’t work!"
  },
  {
    "objectID": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#post_init-assignment-in-a-frozen-dataclass",
    "href": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#post_init-assignment-in-a-frozen-dataclass",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "post_init assignment in a frozen dataclass ✾",
    "text": "post_init assignment in a frozen dataclass ✾\n\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        self.c = self.a + self.b\nfrozen = FrozenDataClass(1,2)\n\nFrozenInstanceError: cannot assign to field 'c'\n\n\nIt doesn’t work! Because the frozen flag will block any assignment even in the __post_init__ method."
  },
  {
    "objectID": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#workaround",
    "href": "blog/posts/2022-04-22-python-dataclass-partiala-immutable.html#workaround",
    "title": "How to achieve Partial Immutability with Python’s dataclass?",
    "section": "workaround",
    "text": "workaround\n\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        object.__setattr__(self, 'c', self.a + self.b)\n        \nfrozen = FrozenDataClass(1,2)\nfrozen.a, frozen.b, frozen.c\n\n(1, 2, 3)\n\n\n\n@dataclass(frozen=True)\nclass FrozenDataClass:\n    a: int\n    b: int\n    \n    def __post_init__(self):\n        super().__setattr__('c', self.a + self.b)\n        \nfrozen = FrozenDataClass(1,2)\nfrozen.a, frozen.b, frozen.c\n\n(1, 2, 3)\n\n\n\nfrozen.c = 3\n\nFrozenInstanceError: cannot assign to field 'c'\n\n\nIt works as expected, the workaround here is using object.__setattr__. The way dataclass achieve immutability is by blocking assignment in the __setattr__ method. This trick works because we are using the object class method instead of the cls method, thus it won’t stop us assign new attribute. More details can be found in Python Standard Doc."
  },
  {
    "objectID": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html",
    "href": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "",
    "text": "I have teamed up with a friend to participate in the Bengali Image Classification Competition. We struggled to get a high rank in the Public leaderboard throughout the competition. In the end, the result is a big surprise to everyone as the leaderboard shook a lot.\nThe final private score was much lower than the public score. It suggests that most participants are over-fitting Public leaderboard."
  },
  {
    "objectID": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#evaluation-metrics",
    "href": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#evaluation-metrics",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics\nThe competition use macro-recall as the evaluation metric. In general, people get >96% recall in training, the tops are even getting >99% recall.\n\n\nCode\npython\nimport numpy as np\nimport sklearn.metrics\n\nscores = []\nfor component in ['grapheme_root', 'consonant_diacritic', 'vowel_diacritic']:\n    y_true_subset = solution[solution[component] == component]['target'].values\n    y_pred_subset = submission[submission[component] == component]['target'].values\n    scores.append(sklearn.metrics.recall_score(\n        y_true_subset, y_pred_subset, average='macro'))\nfinal_score = np.average(scores, weights=[2,1,1])"
  },
  {
    "objectID": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#model-bigger-still-better",
    "href": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#model-bigger-still-better",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Model (Bigger still better)",
    "text": "Model (Bigger still better)\nWe start with xresnet50, which is a relatively small model. As we have the assumption that this classification task is a very standard task, therefore the difference of model will not be the most important one. Thus we pick xresnet50 as it has a good performance in terms of accuracy and train relatively fast.\nNear the end of the competition, we switch to a larger model se-resnext101. It requires triple training time plus we have to scale down the batch size as it does not fit into the GPU memory. Surprisingly (maybe not surprising to everyone), the bigger model did boost the performance more than I expected with ~0.3-0.5% recall. It is a big improvement as the recall is very high (~0.97), in other words, it reduces ~10% error solely by just using a better model, not bad!"
  },
  {
    "objectID": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#augmentation",
    "href": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#augmentation",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Augmentation",
    "text": "Augmentation\nThere are never “enough” data for deep learning, so we always try our best to collect more data. Since we cannot collect more data, we need data augmentation. We start with rotation + scale. We also find MixUp and CutMix is very effective to boost the performance. It also gives us roughly 10% boost initially from 0.96 -> 0.964 recall.\n\nCutMix & MixUp\n\n\n\n\n\nExample of Augmentation\n\n\nMixup is simple, if you know about photography, it is similar to have double exposure of your photos. It overlays two images (cat+dog in this case) by sampling weights. So instead of prediction P(dog) = 1, the new target could become P(dog) = 0.8 and P(cat) = 0.2.\nCutMix shares a similar idea, instead of overlay 2 images, it crops out a certain ratio of the image and replaces it with another one.\nIt always surprises me that these augmented data does not make much sense to a human, but it is very effective to improve model accuracy and reduce overfitting empirically."
  },
  {
    "objectID": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#hydra",
    "href": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#hydra",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Hydra",
    "text": "Hydra\n\n\n\nHydra for configuration composition\n\n\nIt is often a good idea to make your experiment configurable. We use Hydra for this purpose and it is useful to compose different configuration group. By making your hyper-paramters configurable, you can define an experiment by configuration files and run multiple experiments. By logging the configuration with the training statistics, it is easy to do cross-models comparison and find out which configuration is useful for your model.\nI have written an short example for how to use Hydra."
  },
  {
    "objectID": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#wandb",
    "href": "blog/posts/2020-03-21-10-lessons-learnt-from-kaggle-competition.html#wandb",
    "title": "Lesson learnt from Kaggle - Bengali Image Classification Competition",
    "section": "Wandb",
    "text": "Wandb\nwandb (Weight & Biases) does a few things. It provides built-in functions that automatically log all your model statistics, you can also log your custom metrics with simple functions.\n\nCompare the configuration of different experiments to find out the model with the best performance.\nBuilt-in function for logging model weights and gradient for debugging purpose.\nLog any metrics that you want\n\nAll of these combined to make collaboration experience better. It is really important to sync the progress frequently and getting everyone results in a single platform makes these conversations easier.\n\n\n\n\n\nimage.png"
  },
  {
    "objectID": "blog/posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "href": "blog/posts/2021-03-21-full-stack-deep-learning-lecture-01.html",
    "title": "Full Stack Deep Learning Notes - Lecture 01",
    "section": "",
    "text": "Basic Trainer\nhttps://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html\n\n\nCode\nfrom pytorch_lightning import Trainer\n\nimport os\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nimport pytorch_lightning as pl\n\n\nclass SimpleLightningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(28 * 28, 10)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n    def training_step(self, batch, batch_nb):\n        x, y = batch\n        loss = F.cross_entropy(self(x), y)\n        tensorboard_logs = {'train_loss': loss}\n        return {'loss': loss, 'log': tensorboard_logs}\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\n\n\ntrain_loader = DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n\nmnist_model = SimpleLightningModel()\ntrainer = pl.Trainer(gpus=None, progress_bar_refresh_rate=20, max_epochs=1)    \ntrainer.fit(mnist_model, train_loader)\n\nGPU available: False, used: False\nTPU available: None, using: 0 TPU cores\n\n  | Name | Type   | Params\n--------------------------------\n0 | l1   | Linear | 7.9 K \n--------------------------------\n7.9 K     Trainable params\n0         Non-trainable params\n7.9 K     Total params\n0.031     Total estimated model params size (MB)\n\n\n\n\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:51: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\nPlease use self.log(...) inside the lightningModule instead.\n# log on a step or aggregate epoch metric to the logger and/or progress bar (inside LightningModule)\nself.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n  warnings.warn(*args, **kwargs)\n\n\n\n\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:51: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  warnings.warn(*args, **kwargs)\n\n\n1\n\n\nIf you def train_dataloader, Trainer will use it automatically.\n\ndef train_dataloader(self):\n    # REQUIRED\n    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n\n\nSimpleLightningModel.train_dataloader  = train_dataloader\n\n\npl_model = SimpleLightningModel()\ntrainer = Trainer(max_epochs=1)\ntrainer.fit(pl_model)\n\nGPU available: False, used: False\nTPU available: None, using: 0 TPU cores\n\n  | Name | Type   | Params\n--------------------------------\n0 | l1   | Linear | 7.9 K \n--------------------------------\n7.9 K     Trainable params\n0         Non-trainable params\n7.9 K     Total params\n0.031     Total estimated model params size (MB)\n\n\n\n\n\n\n\n\n1\n\n\ntraining_step(), train_dataloader(),configure_optimizers() are essential for LightningModule.\nLifecycle The methods in the LightningModule are called in this order:\n\n__init__\nprepare_data\nconfigure_optimizers\ntrain_dataloader\n\nIf you define a validation loop then val_dataloader\nAnd if you define a test loop: test_dataloader\nYou will find Trainer.fit() automatically do validation and testing for you.\n\ndef validation_step(self, batch, batch_nb):\n    # OPTIONAL\n    x, y = batch\n    y_hat = self(x)\n    return {'val_loss': F.cross_entropy(y_hat, y)}\n\ndef validation_epoch_end(self, outputs):\n    # OPTIONAL\n    avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n    tensorboard_logs = {'val_loss': avg_loss}\n    print(\"Validation Loss: \", avg_loss)\n    return {'val_loss': avg_loss, 'log': tensorboard_logs}\n\ndef val_dataloader(self):\n    # OPTIONAL\n    return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)\n\n\nSimpleLightningModel.validation_step = validation_step\nSimpleLightningModel.validation_epoch_end = validation_epoch_end\nSimpleLightningModel.val_dataloader = val_dataloader\n\n\npl_model = SimpleLightningModel()\ntrainer = Trainer(max_epochs=2)\ntrainer.fit(pl_model)\n\nGPU available: False, used: False\nTPU available: None, using: 0 TPU cores\n\n  | Name | Type   | Params\n--------------------------------\n0 | l1   | Linear | 7.9 K \n--------------------------------\n7.9 K     Trainable params\n0         Non-trainable params\n7.9 K     Total params\n0.031     Total estimated model params size (MB)\n\n\n\n\n\nValidation Loss:  tensor(2.3084)\n\n\n\n\n\n\n\n\nValidation Loss:  tensor(1.1287)\n\n\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:51: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  warnings.warn(*args, **kwargs)\n\n\n1\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you are running the above cell, you will see validation progress bar in action.\n\n\nBy using the trainer you automatically get: * Tensorboard logging * Model checkpointing * Training and validation loop * early-stopping\n\n\nPytorch nn.Module versus pl.LightningModule\n\nimport torch\nimport pytorch_lightning as pl\nfrom torch import nn\n\n\nx = torch.rand((10,10))\nx\n\ntensor([[0.0745, 0.0237, 0.4719, 0.6037, 0.6015, 0.0921, 0.5982, 0.4860, 0.0959,\n         0.5204],\n        [0.2481, 0.2893, 0.5760, 0.3834, 0.6479, 0.0508, 0.5352, 0.5702, 0.4732,\n         0.3867],\n        [0.3467, 0.3321, 0.8570, 0.0983, 0.9210, 0.1848, 0.7397, 0.1350, 0.2646,\n         0.7202],\n        [0.6952, 0.8071, 0.1428, 0.3600, 0.1514, 0.2246, 0.8887, 0.9971, 0.0257,\n         0.5519],\n        [0.7547, 0.7165, 0.3677, 0.6642, 0.9991, 0.6585, 0.8673, 0.5005, 0.1843,\n         0.1360],\n        [0.1809, 0.0794, 0.5101, 0.6751, 0.2822, 0.6695, 0.8085, 0.2127, 0.7562,\n         0.9859],\n        [0.5914, 0.4481, 0.5107, 0.0032, 0.9766, 0.4627, 0.1520, 0.2915, 0.4323,\n         0.3833],\n        [0.6371, 0.7782, 0.7762, 0.4197, 0.2566, 0.7240, 0.0759, 0.9976, 0.6020,\n         0.9528],\n        [0.7674, 0.4044, 0.3497, 0.9784, 0.9318, 0.7313, 0.2962, 0.6555, 0.5570,\n         0.9998],\n        [0.1155, 0.8013, 0.7982, 0.5713, 0.2252, 0.4513, 0.8395, 0.7791, 0.1929,\n         0.7707]])\n\n\n\nclass SimplePytorchModel(nn.Module):\n    ...\n\n\ntorch_model = SimplePytorchModel()\ntorch_model(x)\n\nNotImplementedError: \n\n\nIn python, a NotImplementedError usually appears when you inherit an abstract class, it is a way to tell you that you should implement forward method.\n\nclass SimplePytorchModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10,10)\n    def forward(self,x):\n        return self.linear(x)\n        \ntorch_model = SimplePytorchModel()\ntorch_model(x)\n\ntensor([[-0.1243,  0.2997,  0.0861,  0.1849,  0.7241,  0.2632, -0.0680, -0.2111,\n         -0.2606,  0.0837],\n        [-0.0055,  0.1734,  0.2746,  0.1991,  0.6859,  0.2768,  0.0025, -0.2273,\n         -0.1930,  0.2122],\n        [-0.1407,  0.2008,  0.3773,  0.0956,  0.9796,  0.1915,  0.2936, -0.0837,\n         -0.3146,  0.0808],\n        [-0.0511,  0.1153,  0.2846,  0.2106,  0.7390,  0.0737, -0.1066, -0.3968,\n         -0.3212,  0.2819],\n        [-0.3408,  0.3093,  0.3826,  0.0783,  0.5542,  0.1298, -0.1768, -0.1407,\n         -0.4774,  0.1776],\n        [-0.1892,  0.2563,  0.1489, -0.0091,  0.4639,  0.1332, -0.0166, -0.3798,\n         -0.4021,  0.2960],\n        [-0.1463,  0.0375,  0.4741,  0.0881,  0.5674, -0.0446,  0.1802, -0.2256,\n         -0.3006,  0.0376],\n        [-0.1006, -0.1654,  0.3519,  0.3158,  0.5454, -0.0781,  0.0866, -0.4032,\n         -0.5419,  0.2580],\n        [-0.4006,  0.3089,  0.3450, -0.1411,  0.4353, -0.0416, -0.1630, -0.4652,\n         -0.7266,  0.1949],\n        [-0.1350,  0.0554,  0.1492,  0.4462,  0.8991,  0.2545,  0.1237, -0.1321,\n         -0.4591,  0.2725]], grad_fn=<AddmmBackward>)\n\n\npl.LightningModule is a higher level class for nn.Module.\n\nclass SimpleLightningModel(pl.LightningModule):\n    ...\n    \npl_model = SimpleLightningModel()\npl_model(x)\n\nNotImplementedError: \n\n\nIt shouldn’t surprise you the same error pop out again, after all, pl.LightningModule is a high level wrapper for nn.Module. So we need to implement what is the forward method too. We can confirm this with this line.\n\nissubclass(pl.LightningModule, nn.Module)\n\nTrue\n\n\n\nclass SimpleLightningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(10,10)\n        \n    def forward(self,x):\n        return self.linear(x)\n    \npl_model = SimpleLightningModel()\npl_model(x)\n\ntensor([[-1.9430e-01, -3.2665e-01,  1.5439e-01, -9.5051e-02, -2.6667e-01,\n          7.0515e-01,  5.4318e-01,  4.8522e-02,  2.2087e-01,  4.6927e-02],\n        [-1.9757e-01, -4.1862e-01,  1.0334e-01, -1.7735e-01, -3.7793e-01,\n          7.6570e-01,  5.1128e-01, -5.9839e-04,  2.5192e-01,  9.6547e-02],\n        [-2.1917e-01, -3.4533e-01,  1.6259e-01, -3.4603e-02, -5.8233e-01,\n          7.6317e-01,  4.2289e-01, -5.8673e-02,  1.8833e-01,  9.4830e-02],\n        [ 1.8358e-01, -4.9185e-01,  3.7877e-01, -2.4924e-03,  8.9796e-02,\n          8.3502e-01,  6.2751e-01, -8.9419e-02,  5.8510e-01,  4.9892e-01],\n        [-4.1500e-01, -5.1444e-01,  3.3273e-01, -1.9838e-01, -2.7256e-01,\n          7.2250e-01,  3.3026e-01, -3.0803e-01,  4.8670e-01, -7.5673e-02],\n        [-3.1485e-01, -5.7277e-01,  1.1172e-01,  2.0040e-01, -1.3642e-01,\n          1.1535e+00,  4.7762e-01,  1.8485e-01, -1.2243e-01, -7.5894e-02],\n        [-4.0921e-01, -4.7966e-01,  6.6770e-02, -2.1177e-01, -6.4936e-01,\n          6.5091e-01,  1.9740e-01, -2.5598e-01,  6.5671e-02,  1.9597e-01],\n        [-9.3814e-02, -6.7715e-01,  1.8347e-01, -2.4216e-01, -2.0083e-01,\n          1.1088e+00,  4.1320e-01, -3.5082e-01,  1.6069e-01,  6.4193e-01],\n        [-4.7541e-01, -8.7359e-01,  2.3989e-01, -3.2175e-01, -2.7573e-01,\n          9.9955e-01,  3.8217e-01, -2.8564e-01,  1.1412e-02,  7.2301e-02],\n        [-1.6360e-03, -3.6030e-01,  2.6286e-01,  5.9354e-02,  7.0063e-02,\n          1.0381e+00,  5.0484e-01, -8.8854e-02,  3.9800e-01,  3.4168e-01]],\n       grad_fn=<AddmmBackward>)\n\n\n\n\nPytorch Dataloader versus pl.DataMoudle\nA DataModule implements 5 key methods: * prepare_data (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode, e.g. split data). * setup (things to do on every accelerator in distributed mode, e.g. download data). * train_dataloader the training dataloader. * val_dataloader the val dataloader(s). * test_dataloader the test dataloader(s).\n\n\n\n\n\n\nNote\n\n\n\nWhy do we need to to setup? It’s more a design choice, the benefit of doing so is that the framework takes care how to do distributed training in most efficient way. On the other hand, if you only doing local training on 1 GPU, there is not much benefit of doing so.\n\n\n\n\nTrainer.tune()\n    def tune(self, model, train_dataloader, val_dataloaders, datamodule):\n        # Run auto batch size scaling\n        if self.trainer.auto_scale_batch_size:\n            if isinstance(self.trainer.auto_scale_batch_size, bool):\n                self.trainer.auto_scale_batch_size = 'power'\n            self.scale_batch_size(\n                model,\n                mode=self.trainer.auto_scale_batch_size,\n                train_dataloader=train_dataloader,\n                val_dataloaders=val_dataloaders,\n                datamodule=datamodule,\n            )\n\n        # Run learning rate finder:\n        if self.trainer.auto_lr_find:\n            self.lr_find(model, update_attr=True)\nThe main usage of Trainer.tune() is to automatically find the best learning rate and batch size according to your model.\n\n\nNow Back to our Lab1 (training/run_experiment.py)\nI slightly modified the script so it can be run inside a notebook instead of using argparse. We change these arguments to variable instead.\npython3 training/run_experiment.py --model_class=MLP --data_class=MNIST --max_epochs=5 --gpus=1 --fc1=4 --fc2=8\n\n# Add current directory so we can import the library\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"text_recognizer\"))\n\n\nparser = _setup_parser()\nargs = parser.parse_args([\n    '--model_class',\n    'MLP',\n    '--data_class',\n    'MNIST',\n    '--max_epochs',\n    '5',\n    '--gpus',\n    '0',\n    '--fc1',\n    '4',\n    '--fc2',\n    '8',\n    ])\n\ndata_class = _import_class(f\"text_recognizer.data.{args.data_class}\")\nmodel_class = _import_class(f\"text_recognizer.models.{args.model_class}\")\n\ndata = data_class(args)\nmodel = model_class(data_config=data.config(), args=args)\n\nif args.loss not in ('ctc', 'transformer'):\n    lit_model_class = lit_models.BaseLitModel\n\nif args.load_checkpoint is not None:\n    lit_model = lit_model_class.load_from_checkpoint(args.load_checkpoint, args=args, model=model)\nelse:\n    lit_model = lit_model_class(args=args, model=model)\n\nlogger = pl.loggers.TensorBoardLogger(\"training/logs\")\n\ncallbacks = [pl.callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)]\nargs.weights_summary = \"full\"  # Print full summary of the model\n\ntrainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks, logger=logger, default_root_dir=\"training/logs\")\ntrainer.tune(lit_model, datamodule=data)  # If passing --auto_lr_find, this will set learning rate\ntrainer.fit(lit_model, datamodule=data)\ntrainer.test(lit_model, datamodule=data)\n\ntrainer.tune(lit_model, datamodule=data)  # If passing --auto_lr_find, this will set learning rate\ntrainer.fit(lit_model, datamodule=data)\ntrainer.test(lit_model, datamodule=data)\n\nFirst line try to find the optimal batch size\nSecond line try to trains 5 epochs\nRun test defined in DataModule"
  },
  {
    "objectID": "blog/posts/2020-02-22-python-dynamic-dispatch.html",
    "href": "blog/posts/2020-02-22-python-dynamic-dispatch.html",
    "title": "The missing piece in Python tutorial - What is dispatch why you should care",
    "section": "",
    "text": "Since Python 3.4(PEP443)[https://www.python.org/dev/peps/pep-0443/], generic function is added to Python. This add a new feature that I found much of the exsiting tutorial does not cover it. Such feature is common in other language and is very useful to keep your code concise and clean.\nIn python, you cannot overload a normal function twice for different behavior base on the arguments. For example:\n\ndef foo(number:int ):\n    print('it is a integer')\n    \ndef foo(number: float):\n    print('it is a float')\n\n\nfoo(1)\n\nit is a float\n\n\nThe definition simply get replaced by the second definition. However, with singledispatch, you can define the function behavior base on the type of the argument.\n\nfrom functools import singledispatch\n@singledispatch\ndef foo(number ):\n    print(f'{type(number)}, {number}')\n\n\nfoo(1)\n\n<class 'int'>, 1\n\n\nWe can now register the function for different argument type.\n\n@foo.register(int)\ndef _(data):\n    print('It is a integer!')\n    \n@foo.register(float)\ndef _(data):\n    print('It is a float!')\n\n@foo.register(dict)\ndef _(data):\n    print('It is a dict!')\n\n\nfoo(1.0)\nfoo(1)\nfoo({'1':1})\n\nIt is a float!\nIt is a integer!\nIt is a dict!\n\n\nHow is this possible? Basically there are multiple version of a generic function, singlepatch will pick the correct one base on the type of the first argument.\nIt will fallback to the most generic function if the type of argument is not registered.\n\nfoo([1,2,3])\n\n<class 'list'>, [1, 2, 3]\n\n\nI hope you can see how this is going to be useful. singledispatch limited the usage to the first argument of a function. But we can actually do more than that.\nIn next post I will cover the patch method from fastai will leverage singledispatch more to do multi-dispatch. In python, everything is just an object, even a function itself. So there is no reason why you can only dispatch to a function object. In fact, you could dispatch method to a class too.\n\nFastai @typedispatch\nSingle Dispatch is great, but what if we can do multi dispatch for more than 1 argument?\n\nfrom fastcore.dispatch import  typedispatch, TypeDispatch\n\nLet us first try if this work as expected\n\n@typedispatch\ndef add(x:int, y:int):\n    return x+y\n@typedispatch\ndef add(x:int, y:str):\n    return x + int(y)\n\n\nprint(add(1,2))\nprint(add(1,'2'))\nprint(add('a','a'))\n\n3\n3\na\n\n\n\nadd(1,2)\n\n3\n\n\n\nadd(1,'2')\n\n3\n\n\nBut what if we added something does not define?\n\nadd('2',1)\n\n'2'\n\n\n‘2’? where does it come from? Let’s have a look at the definition of typedispatch and understand how it works.\n\n??typedispatch\nclass DispatchReg:\n    \"A global registry for `TypeDispatch` objects keyed by function name\"\n    def __init__(self): self.d = defaultdict(TypeDispatch)\n    def __call__(self, f):\n        nm = f'{f.__qualname__}'\n        self.d[nm].add(f)\n        return self.d[nm]\n\nIn fact, typedispatch is not even a function, it’s an instance! In python, everything is an object. With the __call__ method, we can use an instance just liek a function. And the typedispatch is just an instance of DispatchReg\n\ntype(typedispatch)\n\nfastcore.dispatch.DispatchReg\n\n\ntypedispatch store a dictionary inside, when you first register your function, it actually store inside a dict. As shown previously, you cannot define the same function twice. But you actually can, because function is nothing but just an object! Let me show you.\n\ndef foo(): return 'foo'\na = foo\ndef foo(): return 'not foo'\nb = foo\n\n\nfoo()\n\n'not foo'\n\n\nfoo() is replaced by the latest definition indeed, but we store a copy of the original function as a variable.\n\na()\n\n'foo'\n\n\n\nb()\n\n'not foo'\n\n\n\nhex(id(a)), hex(id(b))\n\n('0x2b9d28bb5e8', '0x2b9d2ebe048')\n\n\nThe two function is nothing other than two Python object. typedispatch make use of these, when you register a new function, you create an new object and stored inside typedispatch dictionary. It then checks your type annotation and find the corresponding type until it match the issubclass condition.\n\ntypedispatch.d\n\ndefaultdict(fastcore.dispatch.TypeDispatch,\n            {'cast': (object,object) -> cast, 'add': (int,str) -> add\n             (int,int) -> add})\n\n\nSo back to our question, why does add(‘a’,1) return ‘a’? The following explain the reasons. When you call your method, you are really calling the __call__ method inside TypeDispatch, and when the signature is not find, it will simply return the first argument.\ndef __call__(self, *args, **kwargs):\n       ts = L(args).map(type)[:2]\n       f = self[tuple(ts)]\n       if not f: return args[0]\n       if self.inst is not None: f = MethodType(f, self.inst)\n       return f(*args, **kwargs)"
  },
  {
    "objectID": "blog/posts/2020-12-04-kedro-pipeline.html",
    "href": "blog/posts/2020-12-04-kedro-pipeline.html",
    "title": "Introduction to Kedro - pipeline for data science",
    "section": "",
    "text": "Data Scientist often starts their development with a Jupyter Notebook. As the notebook grows larger, it’s inevitable to convert it to a python script. It starts with one file, then another one, and it accumulates quickly. Converting a notebook could be more than just pasting the code in a script. It involves careful thinking and refactoring.\nA pipeline library can be helpful in a few ways: - modular pipeline, it can be executed partially. - easily run in parallel - check for loop dependecies\n\n\nKedro is a development workflow tool that allows you to create portable data pipelines. It applies software engineering best practices to make your data science code reproducible, modular and well-documented. For example, you can easily create a template for new projects, build a documentation site, lint your code and always have an expected structure to find your config and data.\n\nKedro is a lightweight pipeline library without need to setup infracstructure.\n\nIn comparison to Airflow or Luigi, Kedro is much more lightweighted. It helps you to write production-ready code, and let data engineer and data scientist work together with the same code base. It also has good Jupyter support, so data scientist can still use the tool that they are familiar with.\nFunctions and Pipeline\n\n\ndef split_data(data: pd.DataFrame, example_test_data_ratio: float):\n    ...\n    return dict(\n        train_x=train_data_x,\n        train_y=train_data_y,\n        test_x=test_data_x,\n        test_y=test_data_y,\n    )\nNode is the core component of kedro Pipeline. For example, we have a python function that split data into train/test set. A node take 4 arguments. func, inputs, outputs, name. To use this function as a node, we would write something like this.\n\nnode(split_data, inputs=[\"example_iris_data\", \"params:example_test_data_ratio\"],\n                outputs= dict(\n                train_x=\"example_train_x\",\n                train_y=\"example_train_y\",\n                test_x=\"example_test_x\",\n                test_y=\"example_test_y\",\n                ),\n         name=\"split_data\")\nIt’s fairly simple, and it resemble the original function. The only significant difference is, split_data takes a df and float, but in our nodes, it becomes a list of strings. I will explain it in Section 3.2.\n\n\n\nPipeline is nothing more than a list of Node, it helps you to reuse nodes for different pipelines\nPipeline([ṅode(),\n         [node(),\n            ...]])\nHere is an simple Pipeline which does splitting data, train a model, make predictions, and report metrics.\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                split_data,\n                [\"example_iris_data\", \"params:example_test_data_ratio\"],\n                dict(\n                    train_x=\"example_train_x\",\n                    train_y=\"example_train_y\",\n                    test_x=\"example_test_x\",\n                    test_y=\"example_test_y\",\n                ),\n            ),\n            node(\n                train_model,\n                [\"example_train_x\", \"example_train_y\", \"parameters\"],\n                \"example_model\",\n            ),\n            node(\n                predict,\n                dict(model=\"example_model\", test_x=\"example_test_x\"),\n                \"example_predictions\",\n            ),\n            node(report_accuracy, [\"example_predictions\", \"example_test_y\"], None, name='report1'),\n            node(report_accuracy, [\"example_predictions\", \"example_test_y\"], None, name='report2'),\n        ]\n    )\nYou can also use node tags or writing different defined pipeline to reuse your node easily.\n\n\n\nInternally, Kedro always form a graph for your entire pipelines, which can be visaulized with this command.\nkedro viz\nThis starts a web server that visualizes the dependencies of your function, parameters and data,you can also filter some nodes of function with the UI.\n\n\n\nviz\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\nYou can execute your pipeline partially with this command. This with execute your pipeline from A to C except the last Node D.\nkedro run --from-nodes=\"A, B, C\"\nIf you pay attention to this graph, Node B and Node C has no dependency, they only depend on Node A. With kedro, you can parallelize this execution for free by using this command.\nkedro run --parallel\n\n\n\nNow, you have basic understand of what is Node and Pipeline, you also learnt that you can use kedro run command to execute your pipeline with different options. Before I jump into other kedro features, let me explain a bit more about functional programming. This concept is at the heart of data processing library like spark.\nFunctional programming, means using functions to program literally. It may sounds silly, but bear with me.\nPure Function has these characteristics: 1. No side effect, it won’t change state outside of the function scope. 2. If you repeating running the same function with same input(argument), it should give you the same output. 3. Easy to parallel if there is no data dependency\nConsider this simple function that add 1 to your input:\n\ndef func1(x):\n    x=x+1\n\ndef func2(x):\n    return x+1\nvar1 = 1\nvar2 = 1\n\n\nfunc1(var1) # var1=2\nfunc2(var2) # var2=2\nThey both add 1 to your input, so which version is a better function?\nfunc1(var1) # var1=3\nfunc2(var2) # var2=2    \nNow consider if we run this function twice. func1 changes the result to 3, while func2 still give you 2. I argue func2 is better in this case.\nWhy does this matter? Or how is it going to be useful at all? Well, it makes debugging much easier. It is because you only need to debug code inside a function, not 200 lines of code before it. This greatly reduce the complexity that you have to worried about your data. This fundamental principle is what powering the pipeline, and the reason why you can just use kedro run --parallel to parallelize some computation.\nIt will also be easier to write test for function. func1 is harder to test, because you need to consider all possible code path. You may end up need to write verbose test cases like this.\ndef test_case1():\n    func_A()\n    func_B()\n    \ndef test_case2():\n    func_A()\n    func_A()\n    func_B()\nHow does using Kedro helps to achieve this? Think about func1, if it is written as a Node, it will look like this.\nNode(func1, inputs=var1, output=None, name=\"func1\")\nSince it is a Node without any output, it will have no impact to the downstreams. In order to use that variable, you will naturally writing code looks more like func2 instead.\nLet’s look at one more example.\n\nk = 10\ndef func3(x):\n    return x+k\n\n\nfunc3(10)\n\n20\n\n\nNow consider func3, it is a valid Python function. You can run it in a notebook or in a script, but it wouldn’t be possible for a Node, sinec a Node only have access to its input. It will just throw an error to you immediately.\nnode(func3, inputs='x', outputs='some_result', name='func3')\nBy writing nodes, you limit your function to only access variable within its scope. It helps to prevent a lot of bug.\n\n\n\nI hope the examples demonstrate how writing nodes help transform your code towards functional style. In reality, decoupling your functions from a programming is not straight forward.\nConsider this example. \nLook at how data np.nan is changed. This wouldn’t be a problem if we have one program, since we will just passing all variable in memroy, without the step that writing and reading from a file.\nError like these are subtle and dangerous, it may not throw error, but ruining our features quality. We have better chance to catch these error in a small program, but it would be much harder to isolate the issue if we have 1000 lines of code. The sooner you integrate it into your pipeline, the easier the integration is. In fact, we can do better. We could introduce test case for validating data, I would explain more in Section 3.5.\n\n\n\n\n Data Catalog is an API for Dataset. It includes a Data Model from from raw data, feature, to reporting layer and a standard Data I/O API. It integrates with pandas, spark, SQLAlchemy and Cloud Storage.\nTo use Data Catalog, you would first need to define your dataset in the catalog.yml. You will have give it a name and type, denoting whether it is a SQL query or a CSV. Optionally, you can pass in any arguments that are supported from the underlying API as well.\nexample_iris_data:\n  type: pandas.CSVDataSet\n  filepath: data/01_raw/iris.csv\n\n\nLet’s reuse our split_data function. When you create a node that using the split_data function, you would pass in the string of the dataset instead of an actual dataframe, the Reading/Writing operation is handled by Kedro, so you don’t have to write to_csv() or read_csv() yourself.\nparameters.yml\nexample_test_data_ratio: 0.2\nA node using the split_data function.\n\nnode(split_data, inputs=[\"example_iris_data\", \"params:example_test_data_ratio\"],\n                outputs= dict(\n                train_x=\"example_train_x\",\n                train_y=\"example_train_y\",\n                test_x=\"example_test_x\",\n                test_y=\"example_test_y\",\n                ),\n         name=\"split_data\")\nHere the inputs “example_iris_data” is refering to a dataset defined by catalog.yml, kedro will load the csv for you. Same applies for params:example_test_data_ratio.\nBy using catalog and parmaeters, it already makes your program cleaner. You now have a single file to manager all data source, and a single file contains all parameters, which is configurable. Your functions now is parameterized, you can simply change configuration in a single file without going into every possible script to change a number.\nData Catalog abstract away the Data I/O logic from the data processing function.\nIt process data and write a file.\ndef process_data(df):\n    ... # do some processing\n    df.to_csv('xxx.csv')\nIt only process data\ndef process_data(df):\n    ... #do some processing\n    return df\nThis applies the single-responsibility principle (SRP), meaning that your function is only doing one thing at a time. There are many benefits from it, for example, it makes data versioning easier. I will explain this in Section 3.3.\n\n\n\nRemember our we pass in a string to our node, and it will look for the corresponding dataset? What if we do not define it? It could be a lot of work if we need to define everything. Besides, some variable are not needed to be written out as a file, it could just stay as in memory.\nIn fact, kedro use MemroyDataset by default. Which means you could simply pass in a string that is not defined, the string will be use as the name of the variable. There are more useful dataset like CacheDataset, you can find more details in this link.\nhttps://kedro.readthedocs.io/en/stable/kedro.extras.datasets.html\np.s. When using kedro pipeline, you only define the node’s inputs and outputs, but you never defined the order of execution. From my experience, there are pros and cons. The benefits is, your code is less coupled, and due to this, kedro is able to execute your pipeline in parallel whenever possible to speed up your program. However, it means the order of execution is not guaranteed, this may cause unexpected effect. For example, if you are training a machine learning model, it is common to set a random seed at the beginning. Due to the randomness of execution, you may not get identical result, as the order of execution is different everytime, thus the sequence of the random number used is random too. In general this is not a big problem, but if you have a strong need to make sure you have identical output (e.g. regression test), it may cause some trouble and you need to use dummy input and output to force kedro run your pipeline in a specific order."
  },
  {
    "objectID": "blog/posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "href": "blog/posts/2021-11-18-what-can-we-learn-from-shipping-crisis-as-a-data-scientist.html",
    "title": "What can we learn from Shipping Crisis as a Data Scientist?",
    "section": "",
    "text": "You may think the terminal must be busy as hell, so did I, but it is actualy far from the truth. In fact, the port is actually paralyzed. The reason surprised me a lot, it is not because of lacking of driver or empty containers, but yard space. Container are being unloaded from ships, then they are being put at the container yard before they go into depot or being stuffed again.\nOn a high level, it is caused by a negative feedback loop which COVID probably contributed a lot, as it caused a lot of disruption to the supply chain.\n\nPort Congestion -> Containers pilled up at container yard since it is waiting to be loaded on ship\nContainer yard space is taken up by cotnainers, less space is available\nA container need to be put on a chassis before it is loaded, but as the container yard is full, empty containers stuck on the chassis and they need to be unloaded before you put a stuffed container.\nLess Chassis is available to load stuff, so it further slow down the process\nThe loop complete and it starts from 1 again\n\n\n\n\nPort Congestion Feedback Loop\n\n\nThis is a simplified story, you can find more details from this twitter thread from flexport’s CEO Ryan. There are more constraints that making this load/unload process inefficient, so the whole process is jammed. Think about a restaurant with limited amount of trays, you need to get a tray if you want to get food. But because there are too many customers, it jammed the door . So there are many customers holding an empty tray while many food are waiting to be served.\nRyan point out a very important lesson here, that is, you need to choose your bottleneck, and it should really be the capital intensive assets. Going back to our restaurant’s analogy, chef and space is probably the most expensive assets, so we should try to keep the utilization high. A simple solution is to buy more trays, so that it won’t be jammed. Ofcourse, you can also find a larger space, build a bigger door, but that will cost you more money too.\nFor shipping, the terminal’s crane should be the most capital intensive, so we should try our best to keep it working 24/7 to digest the terminal queue.\nThis is a simple idea yet it is powerful and it strikes me hard. As a data scientist, I work on optimization problem. To maximize the output of a system, we can use linear programming. When we are solving this problem, we are asking question like this.\n\nGiven x1 Terminals, x2 drivers, x3 containers, x4 ships, what is the maximize output of this system and how do you arrange them to achieve so?\n\nHowever, if you are a product/business analyst, a better question may be > What is the output of this system if I add more container yard space?\nBy changing the input of the system, you may achieve much better result. But as a data scientist, we often stuck in a mode that how do we optimize x metrics with these features. So we may end up spending months and try to schedule ships and driver perfectly to load 10% more container, but you can actually increase loading efficiency by 50% simply by adding more yard space. It feels like cheating as a scientific question, since this is not we asked originally, but this happened a lot in a business context.\nWe are not trying to find the best algorithm to solve a problem, the algorithm is just one way of doing it. We may get surprising result by just tweaking the asked question a little bit.\nI am curious about what is the limiting factor in our current supply chain system, and how sensitive it is to the environment. Is forecasting & optimization the right way to do it? Do we actually need a precise forecast or we can have a bit of redundancy (like in this case, having extra yard space which could be a waste but improve the system robustness)? This is questions that we need to ask ourselves constantly, as the true question is often not asked, but explored after lots of iterations. We need to, and we have to ask the right question, and that is an art more elegant than an algorithm in my opinion.\nI do not know if Ryan’s word are 100% true, but it reminds me an important lesson. The right solution (question) may be simple, but it may not be obvious. Have we exploited all the simple solution before we went nuts with fancy algorithms?\np.s. Apologised as I don’t have time to proofread but simply try to write down the snapshot of my current mind [2021-11-18]\n\nReference\n{% twitter https://twitter.com/typesfast/status/1451543776992845834?s=20 %} https://twitter.com/typesfast/status/1451543776992845834?s=20 https://www.facebook.com/669645890/posts/10159859049175891/ unroll version: https://threadreaderapp.com/thread/1451543776992845834.html"
  },
  {
    "objectID": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "href": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "",
    "text": "GitHub: https://github.com/noklam/notadatascientist/tree/master/demo/hydra-example\nMachine learning project involves large number of hyperparmeters. In many case you could have multiple config, e.g. differnet dataset, database connection, train/test mode. hydra provide a simple Command Line Interface that is useful for composing different experiment configs. In essence, it compose different files to a large config setting. It offers you the common Object Oriented Programming with YAML file. Allow you to have clear structure of configurations.\nAssume you have a config.yaml like this, where run_mode and hyperparmeter are separate folder to hold different choice of parameters. You can set defaults for them with the following structure."
  },
  {
    "objectID": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "href": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#folder-structure",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "Folder Structure",
    "text": "Folder Structure\nconfig.yaml\ndemo.py\nrun_mode\n  - train.yaml\n  - test.yaml\nhyperparmeter\n  - base.yaml"
  },
  {
    "objectID": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "href": "blog/posts/2020-02-08-Config-Composition-with-Hydra-for-Machine-Learning-Experiments.html#config.yaml",
    "title": "Hydra - Config Composition for Machine Learning Project",
    "section": "config.yaml",
    "text": "config.yaml\ndefaults:\n - run_mode: train\n - hyperparameter: base\nThe benefit of using such approach is that it makes comparsion of experiments much easier. Instead of going through the parameters list, you only focus on the argument(the difference). It helps organize machine learning results and ease a lot of pain in tracking the model performance.\nimport hydra\nfrom omegaconf import DictConfig\n@hydra.main(config_path=\"config.yaml\")\ndef my_app(cfg : DictConfig) -> None:\n    print(cfg.pretty())\nif __name__ == \"__main__\":\n    my_app()\npython demo.py \ngamma: 0.01\nlearning_rate: 0.01\nrun_mode: train\nweek: 8\nFor example, with a simple example with 4 parameters only, you can simply run the experiment with default"
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html",
    "href": "blog/posts/2019-01-01-codespace-template.html",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "",
    "text": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming?"
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#introduction",
    "href": "blog/posts/2019-01-01-codespace-template.html#introduction",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Introduction",
    "text": "Introduction\n\nLiterate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1\n\nWhen I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2\n\nIt can be difficult to compile source code from notebooks.\nIt can be difficult to diff and use version control with notebooks because they are not stored in plain text.\nIt is not clear how to automatically generate documentation from notebooks.\nIt is not clear how to properly run tests suites when writing code in notebooks.\n\nMy skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used.\nAs a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/e2e_small.mp4” %}"
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#features-of-nbdev",
    "href": "blog/posts/2019-01-01-codespace-template.html#features-of-nbdev",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Features of nbdev",
    "text": "Features of nbdev\nAs discussed in the docs, nbdev provides the following features:\n\nSearchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free.\nPython modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables.\nPip and Conda installers.\nTests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions.\nNavigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks.\n\nSince you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev."
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#github-codespaces",
    "href": "blog/posts/2019-01-01-codespace-template.html#github-codespaces",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "GitHub Codespaces",
    "text": "GitHub Codespaces\nThanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features:\n\nA full VS Code IDE.\nAn environment that has files from the repository mounted into the environment, along with your GitHub credentials.\nA development environment with dependencies pre-installed, backed by Docker.\nThe ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site.\nA shared file system, which facilitates editing code in one browser tab and rendering the results in another.\n… and more.\n\nCodespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming."
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#a-demo-of-nbdev-codespaces",
    "href": "blog/posts/2019-01-01-codespace-template.html#a-demo-of-nbdev-codespaces",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "A demo of nbdev + Codespaces",
    "text": "A demo of nbdev + Codespaces\nThis demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/1_open.mp4” %}\n\n\n\nIf you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev):\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/2_verify.mp4” %}\n\n\n\nAdditionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/3_nb_small.mp4” %}\n\n\n\nIn this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below:\n{% include video.html url=“https://github.com/machine-learning-apps/demo-videos/raw/master/codespaces-nbdev/4_reload_small.mp4” %}\n\n\n\nThis is amazing! With a click of a button, I was able to:\n\nLaunch an IDE with all dependencies pre-installed.\nLaunch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000.\nAutomatically update the docs and modules every time I make a change to a Jupyter notebook.\n\nThis is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs."
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#give-it-a-try-for-yourself",
    "href": "blog/posts/2019-01-01-codespace-template.html#give-it-a-try-for-yourself",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Give It A Try For Yourself",
    "text": "Give It A Try For Yourself\nTo try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev."
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#you-can-write-blogs-with-notebooks-too",
    "href": "blog/posts/2019-01-01-codespace-template.html#you-can-write-blogs-with-notebooks-too",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "You Can Write Blogs With Notebooks, Too!",
    "text": "You Can Write Blogs With Notebooks, Too!\nThis blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks."
  },
  {
    "objectID": "blog/posts/2019-01-01-codespace-template.html#additional-resources",
    "href": "blog/posts/2019-01-01-codespace-template.html#additional-resources",
    "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nThe GitHub Codepaces site.\nThe official docs for Codespaces.\nThe nbdev docs.\nThe nbdev GitHub repo.\nfastpages: The project used to write this blog.\nThe GitHub repo fastai/fastcore, which is what we used in this blog post as an example."
  },
  {
    "objectID": "blog/posts/2020-11-10-pandas memory optimization.html",
    "href": "blog/posts/2020-11-10-pandas memory optimization.html",
    "title": "Optimizing pandas - Reducing 90% memory footprint - updated version",
    "section": "",
    "text": "TWO options to automatically optimize pandas\n\nWe can check some basic info about the data with pandas .info() function\n\ndf_gamelogs.info(memory_usage='deep')\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 171907 entries, 0 to 171906\nColumns: 161 entries, date to acquisition_info\ndtypes: float64(77), int64(6), object(78)\nmemory usage: 860.5 MB\n\n\nWe can see the data has 171907 rows and 161 columns and 859.4 MB memory. Let’s see how much we can optimize dtype_diet.\n\nproposed_df = report_on_dataframe(df_gamelogs, unit=\"MB\")\nproposed_df\n\n\n\n\n\n  \n    \n      \n      Current dtype\n      Proposed dtype\n      Current Memory (MB)\n      Proposed Memory (MB)\n      Ram Usage Improvement (MB)\n      Ram Usage Improvement (%)\n    \n    \n      Column\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      date\n      int64\n      int32\n      671.574219\n      335.818359\n      335.755859\n      49.995347\n    \n    \n      number_of_game\n      int64\n      int8\n      671.574219\n      84.001465\n      587.572754\n      87.491857\n    \n    \n      day_of_week\n      object\n      category\n      5036.400391\n      84.362793\n      4952.037598\n      98.324939\n    \n    \n      v_name\n      object\n      category\n      5036.400391\n      174.776367\n      4861.624023\n      96.529736\n    \n    \n      v_league\n      object\n      category\n      4952.461426\n      84.359375\n      4868.102051\n      98.296617\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      h_player_9_id\n      object\n      category\n      4955.471680\n      412.757324\n      4542.714355\n      91.670675\n    \n    \n      h_player_9_name\n      object\n      category\n      5225.463379\n      421.197266\n      4804.266113\n      91.939523\n    \n    \n      h_player_9_def_pos\n      float64\n      float16\n      671.574219\n      167.940430\n      503.633789\n      74.993020\n    \n    \n      additional_info\n      object\n      category\n      2714.671875\n      190.601074\n      2524.070801\n      92.978854\n    \n    \n      acquisition_info\n      object\n      category\n      4749.209961\n      84.070801\n      4665.139160\n      98.229794\n    \n  \n\n161 rows × 6 columns\n\n\n\n\nnew_df = optimize_dtypes(df_gamelogs, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA\n\n\nprint(f'Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB')\nprint(f'Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB')\n\nOriginal df memory: 860.500262260437 MB\nPropsed df memory: 79.04368686676025 MB\n\n\n\n# sell_prices.csv.zip Source data: https://www.kaggle.com/c/m5-forecasting-uncertainty/\ndf = pd.read_csv('../data/sell_prices.csv.zip')\n\n\nproposed_df = report_on_dataframe(df, unit=\"MB\")\nnew_df = optimize_dtypes(df, proposed_df) # Avoid Type conversion error from int64 to int 8 with NA\n\n\nprint(f'Original df memory: {df_gamelogs.memory_usage(deep=True).sum()/1024/1024} MB')\nprint(f'Propsed df memory: {new_df.memory_usage(deep=True).sum()/1024/1024} MB')\n\nOriginal df memory: 860.500262260437 MB\nPropsed df memory: 85.09655094146729 MB\n\n\n\n## hide\n## collapse-hide"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "",
    "text": "Missing Subtitle (plotnine)\nMissing Style"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-example",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-example",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "Plotnine Example",
    "text": "Plotnine Example\n\n(ggplot(mtcars, aes('wt', 'mpg', color='factor(gear)'))\n + geom_point()\n + stat_smooth(method='lm')\n + facet_wrap('~gear'))\n\n\n\n\n<ggplot: (-9223371941312347920)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\nline_df <- gapminder %>%\n  filter(country == \"Malawi\") \n\n#Make plot\nline <- ggplot(line_df, aes(x = year, y = lifeExp)) +\n  geom_line(colour = \"#1380A1\", size = 1) +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  bbc_style() +\n  labs(title=\"Living longer\",\n       subtitle = \"Life expectancy in Malawi 1952-2007\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n(ggplot(line_df, aes(x='year', y='lifeExp')) +\n geom_line(colour='#1380A1', size=1) +\n geom_hline(yintercept = 0, size = 1, colour='#333333') +\n labs(title='Living longer', \n     subtitle = 'Life expectancy in Malawi 1952-2007')\n )\n\n\n\n\n<ggplot: (-9223371941310406772)>\n\n\n\n## altair\nline = (alt.Chart(line_df).mark_line().encode(\nx='year',\ny='lifeExp')\n.properties(title={'text': 'Living Longer',\n                   'subtitle': 'Life expectancy in Malawi 1952-2007'})\n) \n\n# hline\noverlay = overlay = pd.DataFrame({'y': [0]})\nhline = alt.Chart(overlay).mark_rule(color='#333333', strokeWidth=3).encode(y='y:Q')\n\nline + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\n## altair\nline = (alt.Chart(line_df).mark_line().encode(\nx='year',\ny='lifeExp')\n.properties(title={'text': 'Living Longer',\n                   'subtitle': 'Life expectancy in China 1952-2007'})\n) \n\n# hline\noverlay = overlay = pd.DataFrame({'lifeExp': [0]})\nhline = alt.Chart(overlay).mark_rule(color='#333333', strokeWidth=3).encode(y='lifeExp:Q')\n\nline + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-1",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-1",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#Prepare data\nmultiple_line_df <- gapminder %>%\n  filter(country == \"China\" | country == \"United States\") \n\n#Make plot\nmultiple_line <- ggplot(multiple_line_df, aes(x = year, y = lifeExp, colour = country)) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  scale_colour_manual(values = c(\"#FAAB18\", \"#1380A1\")) +\n  bbc_style() +\n  labs(title=\"Living longer\",\n       subtitle = \"Life expectancy in China and the US\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-1",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-1",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# Make plot\nmultiline = (\n    ggplot(multiline_df, aes(x='year', y='lifeExp', colour='country')) +\n    geom_line(colour=\"#1380A1\", size=1) +\n    geom_hline(yintercept=0, size=1, color=\"#333333\") +\n    scale_colour_manual(values=[\"#FAAB18\", \"#1380A1\"]) +\n      bbc_style() +\n    labs(title=\"Living longer\",\n         subtitle=\"Life expectancy in China 1952-2007\"))\nmultiline\n\nfindfont: Font family ['Helvetica'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['Helvetica'] not found. Falling back to DejaVu Sans.\n\n\n\n\n\n<ggplot: (-9223371941310014864)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-1",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-1",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\nmultiline_altair = (alt.Chart(multiline_df).mark_line().encode(\nx='year',\ny='lifeExp',\ncolor='country')\n.properties(title={'text': 'Living Longer',\n                   'subtitle': 'Life expectancy in China 1952-2007'})\n) \n\n# hline\noverlay = overlay = pd.DataFrame({'lifeExp': [0]})\nhline = alt.Chart(overlay).mark_rule(color='#333333', strokeWidth=3).encode(y='lifeExp:Q')\n\nmultiline_altair + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-2",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-2",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\nbars_ggplot = (ggplot(bar_df, aes(x='country', y='lifeExp')) +\n  geom_bar(stat=\"identity\",\n           position=\"identity\",\n           fill=\"#1380A1\") +\n  geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n#   bbc_style() +\n  labs(title=\"Reunion is highest\",\n       subtitle=\"Highest African life expectancy, 2007\"))\n\nbars_ggplot\n\n\n\n\n<ggplot: (-9223371941310355340)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-2",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-2",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\nbars_altair = (alt.Chart(bar_df).mark_bar().encode(\nx='country',\ny='lifeExp',\n# color='country'\n)\n.properties(title={'text': 'Reunion is highest',\n                   'subtitle': 'Highest African life expectancy, 2007'})\n) \n\nbars_altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#data-preprocessing",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#data-preprocessing",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n\n## collapse-hide\nstacked_bar_df = (\n    gapminder.query(' year == 2007')\n    .assign(\n        lifeExpGrouped=lambda x: pd.cut(\n            x['lifeExp'],\n            bins=[0, 50, 65, 80, 90],\n            labels=[\"under 50\", \"50-65\", \"65-80\", \"80+\"]))\n    .groupby(\n        ['continent', 'lifeExpGrouped'], as_index=True)\n    .agg({'pop': 'sum'})\n    .rename(columns={'pop': 'continentPop'})\n    .reset_index()\n)\nstacked_bar_df['lifeExpGrouped'] = pd.Categorical(stacked_bar_df['lifeExpGrouped'], ordered=True)\n\nstacked_bar_df.head(6)\n\n\n\n\n\n  \n    \n      \n      continent\n      lifeExpGrouped\n      continentPop\n    \n  \n  \n    \n      0\n      Africa\n      under 50\n      376100713.0\n    \n    \n      1\n      Africa\n      50-65\n      386811458.0\n    \n    \n      2\n      Africa\n      65-80\n      166627521.0\n    \n    \n      3\n      Africa\n      80+\n      NaN\n    \n    \n      4\n      Americas\n      under 50\n      NaN\n    \n    \n      5\n      Americas\n      50-65\n      8502814.0"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-3",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-3",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#prepare data\nstacked_df <- gapminder %>% \n  filter(year == 2007) %>%\n  mutate(lifeExpGrouped = cut(lifeExp, \n                    breaks = c(0, 50, 65, 80, 90),\n                    labels = c(\"Under 50\", \"50-65\", \"65-80\", \"80+\"))) %>%\n  group_by(continent, lifeExpGrouped) %>%\n  summarise(continentPop = sum(as.numeric(pop)))\n\n#set order of stacks by changing factor levels\nstacked_df$lifeExpGrouped = factor(stacked_df$lifeExpGrouped, levels = rev(levels(stacked_df$lifeExpGrouped)))\n\n#create plot\nstacked_bars <- ggplot(data = stacked_df, \n                       aes(x = continent,\n                           y = continentPop,\n                           fill = lifeExpGrouped)) +\n  geom_bar(stat = \"identity\", \n           position = \"fill\") +\n  bbc_style() +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_viridis_d(direction = -1) +\n  geom_hline(yintercept = 0, size = 1, colour = \"#333333\") +\n  labs(title = \"How life expectancy varies\",\n       subtitle = \"% of population by life expectancy band, 2007\") +\n  theme(legend.position = \"top\", \n        legend.justification = \"left\") +\n  guides(fill = guide_legend(reverse = TRUE))"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-3",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-3",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# create plot\nstacked_bar_ggplot = (\n    ggplot(stacked_bar_df,\n           aes(x='continent',\n               y='continentPop',\n               fill='lifeExpGrouped')\n           ) +\n    geom_bar(stat=\"identity\",\n             position=\"fill\") +\n    #   bbc_style() +\n    scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l]) +\n    scale_fill_cmap_d(direction=-1) +  # scale_fill_viridis_d\n    geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n    labs(title=\"How life expectancy varies\",\n         subtitle=\"% of population by life expectancy band, 2007\") +\n\n    guides(fill=guide_legend(reverse=True)))\n\nstacked_bar_ggplot\n\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction`\n  warn(msg.format(self.__class__.__name__, k), PlotnineWarning)\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values.\n  data = self.position.setup_data(self.data, params)\n\n\n\n\n\n<ggplot: (-9223371941310320660)>\n\n\n\n# create plot\nstacked_bar_ggplot = (\n    ggplot(stacked_bar_df,\n           aes(x='continent',\n               y='continentPop',\n               fill='lifeExpGrouped')\n           ) +\n    geom_bar(stat=\"identity\",\n             position=\"fill\") +\n    #   bbc_style() +\n    scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l]) +\n    scale_fill_cmap_d(direction=-1) +  # scale_fill_viridis_d\n    geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n    labs(title=\"How life expectancy varies\",\n         subtitle=\"% of population by life expectancy band, 2007\") +\n\n    guides(fill=guide_legend(reverse=True)))\n\nstacked_bar_ggplot\n\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\scales\\scale.py:91: PlotnineWarning: scale_fill_cmap_d could not recognise parameter `direction`\n  warn(msg.format(self.__class__.__name__, k), PlotnineWarning)\nC:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\layer.py:433: PlotnineWarning: position_stack : Removed 7 rows containing missing values.\n  data = self.position.setup_data(self.data, params)\n\n\n\n\n\n<ggplot: (-9223371941310406808)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-3",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-3",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\nstacked_bar_altair = (\n    alt.Chart(stacked_bar_df)\n    .mark_bar()\n    .encode(x='continent',\n            y=alt.Y('continentPop', stack='normalize',\n                    axis=alt.Axis(format='%')),\n            fill=alt.Fill('lifeExpGrouped', scale=alt.Scale(scheme='viridis')))\n    .properties(title={'text': 'How life expectancy varies',\n                       'subtitle': '% of population by life expectancy band, 2007'}\n                )\n)\n\noverlay = overlay = pd.DataFrame({'continentPop': [0]})\nhline = alt.Chart(overlay).mark_rule(\n    color='#333333', strokeWidth=2).encode(y='continentPop:Q')\n\n\nstacked_bar_altair + hline"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-4",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-4",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#Prepare data\ngrouped_bar_df <- gapminder %>%\n  filter(year == 1967 | year == 2007) %>%\n  select(country, year, lifeExp) %>%\n  spread(year, lifeExp) %>%\n  mutate(gap = `2007` - `1967`) %>%\n  arrange(desc(gap)) %>%\n  head(5) %>%\n  gather(key = year, \n         value = lifeExp,\n         -country,\n         -gap) \n  \n#Make plot\ngrouped_bars <- ggplot(grouped_bar_df, \n                       aes(x = country, \n                           y = lifeExp, \n                           fill = as.factor(year))) +\n  geom_bar(stat=\"identity\", position=\"dodge\") +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  bbc_style() +\n  scale_fill_manual(values = c(\"#1380A1\", \"#FAAB18\")) +\n  labs(title=\"We're living longer\",\n       subtitle = \"Biggest life expectancy rise, 1967-2007\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-4",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-4",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# Make plot\ngrouped_bars_ggplot = (ggplot(grouped_bar_df,\n                       aes(x='country',\n                           y='lifeExp',\n                           fill='year')) +\n                geom_bar(stat=\"identity\", position=\"dodge\") +\n                geom_hline(yintercept=0, size=1, colour=\"#333333\") +\n#                 bbc_style() +\n                scale_fill_manual(values=(\"#1380A1\", \"#FAAB18\")) +\n                labs(title=\"We're living longer\",\n                     subtitle=\"Biggest life expectancy rise, 1967-2007\"))\n\ngrouped_bars_ggplot\n\n\n\n\n<ggplot: (-9223371941310352504)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-4",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-4",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\ngrouped_bars_altair = (\n    alt.Chart(grouped_bar_df)\n    .mark_bar()\n    .encode(x='year:N',\n            y='lifeExp',\ncolor=alt.Color('year:N', scale=alt.Scale(range=[\"#1380A1\", \"#FAAB18\"])),\n           column='country:N')\n    .properties(title={'text': \"We're living longe\",\n                       'subtitle': 'Biggest life expectancy rise, 1967-2007'}\n                )\n)\n\ngrouped_bars_altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-5",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-5",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\n\n# Remove the Legend\nmultiline + guides(colour=False)\n\n\n\n\n<ggplot: (-9223371941310211164)>\n\n\n\nmultiline + theme(legend_position = \"none\")\n\n\n\n\n<ggplot: (-9223371941308840576)>\n\n\n\nfrom plotnine import unit\n\nImportError: cannot import name 'unit' from 'plotnine' (C:\\Users\\CHANNO.OOCLDM\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\plotnine\\__init__.py)\n\n\n\n# Change position of the legend\nx=multiline + theme(\n  axis_ticks_major_x = element_line(color = \"#333333\"), \n  axis_ticks_length =  0.26)\n\nx\n\n\n\n\n<ggplot: (-9223371941310025320)>"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-5",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-5",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-5",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-5",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-6",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-6",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-6",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-6",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-6",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-6",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-7",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-7",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-7",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-7",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-7",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-7",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-8",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-8",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-8",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-8",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-8",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-8",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-9",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-9",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-9",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-9",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-9",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#ggplot-9",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "ggplot",
    "text": "ggplot\n#Prepare data\ndumbbell_df <- gapminder %>%\n  filter(year == 1967 | year == 2007) %>%\n  select(country, year, lifeExp) %>%\n  spread(year, lifeExp) %>%\n  mutate(gap = `2007` - `1967`) %>%\n  arrange(desc(gap)) %>%\n  head(10)\n\nggplot(hist_df, aes(lifeExp)) +\n  geom_histogram(binwidth = 5, colour = \"white\", fill = \"#1380A1\") +\n  geom_hline(yintercept = 0, size = 1, colour=\"#333333\") +\n  bbc_style() +\n  scale_x_continuous(limits = c(35, 95),\n                     breaks = seq(40, 90, by = 10),\n                     labels = c(\"40\", \"50\", \"60\", \"70\", \"80\", \"90 years\")) +\n  labs(title = \"How life expectancy varies\",\n       subtitle = \"Distribution of life expectancy in 2007\")"
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-10",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#plotnine-10",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "plotnine",
    "text": "plotnine\nNot available with plotnine."
  },
  {
    "objectID": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-10",
    "href": "blog/posts/2020-04-13-recreating-the-bbc-graphs-in-python-plotnine-altair.html#altair-10",
    "title": "Recreating the BBC style graphic in Python - plotnine and altair",
    "section": "altair",
    "text": "altair\n\ndumbbell_chart_altair = (\nalt.Chart(dumbbell_chart_df).\n    mark_rule()\n)\n\ndumbbell_chart_altair\n\n()"
  },
  {
    "objectID": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "href": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html",
    "title": "deepcopy, LGBM and pickle",
    "section": "",
    "text": "To start with, let’s look at some code to get some context."
  },
  {
    "objectID": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "href": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#deepcopy-or-no-copy",
    "title": "deepcopy, LGBM and pickle",
    "section": "deepcopy or no copy?",
    "text": "deepcopy or no copy?\n\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom copy import deepcopy\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\nprint(\"Parameters of the model: \", lgbm.params)\n\nParameters of the model:  {'objective': 'regression', 'verbose': -1, 'num_leaves': 3, 'num_iterations': 1, 'early_stopping_round': None}\n\n\n\n## Deep copy will missing params\nnew_model = deepcopy(lgbm)\n\nFinished loading model, total used 1 iterations\n\n\nYou would expect new_model.parameters return the same dict right? Not quite.\n\nprint(\"Parameters of the copied model: \", new_model.params)\n\nParameters of the copied model:  {}\n\n\nSurprise, surprise. It’s an empty dict, where did the parameters go? To dive deep into the issue, let’s have a look at the source code of deepcopy to understand how does it work.\nreference: https://github.com/python/cpython/blob/e8e341993e3f80a3c456fb8e0219530c93c13151/Lib/copy.py#L128\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    ... # skip some irrelevant code  \n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier is not None:\n        y = copier(x, memo)\n    else:\n        if issubclass(cls, type):\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier is not None:\n                y = copier(memo)\n            else:\n                ... # skip irrelevant code\n\n    # If is its own copy, don't memoize.\n    if y is not x:\n        memo[d] = y\n        _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\nIn particular, line 17 is what we care.\ncopier = getattr(x, \"__deepcopy__\", None)\nIf a particular class has implement the __deepcopy__ method, deepcopy will try to invoke that instead of the standard copy. The following dummy class should illustrate this clearly.\n\nclass DummyClass():\n    def __deepcopy__(self, _):\n        print('Just hanging around and not copying.')\n\n\no = DummyClass()\ndeepcopy(o)\n\nJust hanging around and not copying.\n\n\na lightgbm model is actually a Booster object and implement its own __deepcopy__. It only copy the model string but nothing else, this explains why deepcopy(lgbm).paramters is an empty dictionary.\n def __deepcopy__(self, _): \n     model_str = self.model_to_string(num_iteration=-1) \n     booster = Booster(model_str=model_str) \n     return booster \nReference: https://github.com/microsoft/LightGBM/blob/d6ebd063fff7ff9ed557c3f2bcacc8f9456583e6/python-package/lightgbm/basic.py#L2279-L2282\nOkay, so why lightgbm need to have an custom implementation? I thought this is a bug, but turns out there are some deeper reason behind this. I created an issue on GitHub.\nhttps://github.com/microsoft/LightGBM/issues/4085 Their response is > Custom deepcopy is needed to make Booster class picklable."
  },
  {
    "objectID": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "href": "blog/posts/2021-03-19-deepcopy-lightgbm-and-🥒pickles.html#italian-bmt-lettuce-tomato-and-some-pickles-please",
    "title": "deepcopy, LGBM and pickle",
    "section": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please",
    "text": "🥖Italian BMT, 🥬Lettuce 🍅 tomato and some 🥒pickles please\nWhat does pickle really is? and what makes an object pickable?\n\nPython Pickle is used to serialize and deserialize a python object structure. Any object on python can be pickled so that it can be saved on disk.\n\nSerialization roughly means translating the data in memory into a format that can be stored on disk or sent over network. It’s like ordering a chair from Ikea, they will send you a box, but not a chair.\nThe process of decomposing the chair and put it into a box is serialization, while putting it together is deserialization. With pickle terms, we called it Pickling and Unpickling.\n\n\n\n\n\ndeserialize and serialize\n\n\n\nWhat is Pickle\nPickle is a protocol for Python, you and either pickling a Python object to memory or to file.\n\nimport pickle\n\n\nd = {'a': 1}\npickle_d = pickle.dumps(d)\npickle_d\n\nb'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\n\n\nThe python dict is now transfrom into a series of binary str, this string can be only understand by Python. We can also deserialize a binary string back to a python dict.\n\nbinary_str = b'\\x80\\x04\\x95\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x01a\\x94K\\x01s.'\npickle.loads(binary_str)\n\n{'a': 1}\n\n\nReference: https://www.python.org/dev/peps/pep-0574/#:~:text=The%20pickle%20protocol%20was%20originally%20designed%20in%201995,copying%20temporary%20data%20before%20writing%20it%20to%20disk.\n\n\nWhat makes something picklable\nFinally, we come back to our initial questions. > What makes something picklable? Why lightgbm need to have deepcopy to make the Booster class picklable?\n\nWhat can be pickled and unpickled? The following types can be pickled:\n* None, True, and False\n* integers, floating point numbers, complex numbers\n* strings, bytes, bytearrays\n* tuples, lists, sets, and dictionaries containing only picklable objects\n* functions defined at the top level of a module (using def, not lambda)\n* built-in functions defined at the top level of a module\n* classes that are defined at the top level of a module\n\nSo pretty much common datatype, functions and classes are picklable. Let’s see without __deepcopy__, the Booster class is not serializable as it claims.\n\nimport lightgbm\nfrom lightgbm import Booster\ndel Booster.__deepcopy__\n\nparams = {\n'objective': 'regression',\n'verbose': -1,\n'num_leaves': 3\n}\n\nX = np.random.rand(100,2)\nY = np.ravel(np.random.rand(100,1))\nlgbm = lgb.train(params, lgb.Dataset(X,label=Y),num_boost_round=1)\n\n\ndeepcopy_lgbm = deepcopy(lgbm)\nlgbm.params, deepcopy_lgbm.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\npickle.dumps(deepcopy_lgbm) == pickle.dumps(lgbm)\n\nTrue\n\n\n\nunpickle_model = pickle.loads(pickle.dumps(deepcopy_lgbm))\nunpickle_deepcopy_model = pickle.loads(pickle.dumps(lgbm))\n\n\nunpickle_model.params, unpickle_deepcopy_model.params\n\n({'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None},\n {'objective': 'regression',\n  'verbose': -1,\n  'num_leaves': 3,\n  'num_iterations': 1,\n  'early_stopping_round': None})\n\n\n\nunpickle_model.model_to_string() == unpickle_deepcopy_model.model_to_string()\n\nTrue\n\n\n\nunpickle_deepcopy_model.predict(X)\n\narray([0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.49029787, 0.48439803, 0.50141491, 0.50141491, 0.50141491,\n       0.48439803, 0.50141491, 0.48439803, 0.49029787, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.49029787, 0.49029787,\n       0.49029787, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.49029787, 0.50141491, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.48439803, 0.49029787,\n       0.48439803, 0.48439803, 0.50141491, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.48439803, 0.50141491,\n       0.49029787, 0.48439803, 0.50141491, 0.49029787, 0.49029787,\n       0.50141491, 0.50141491, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.48439803,\n       0.48439803, 0.50141491, 0.50141491, 0.49029787, 0.50141491,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.50141491,\n       0.50141491, 0.48439803, 0.49029787, 0.48439803, 0.48439803,\n       0.50141491, 0.49029787, 0.50141491, 0.50141491, 0.49029787,\n       0.48439803, 0.49029787, 0.48439803, 0.48439803, 0.48439803,\n       0.48439803, 0.48439803, 0.48439803, 0.50141491, 0.49029787])\n\n\n\n\nLast Word\nWell…. It seems actually picklable? I may need to investigate the issue a bit more. For now, the __deepcopy__ does not seems to be necessary.\nI tried to dig into lightgbm source code and find this potential related issue. https://github.com/microsoft/LightGBM/blame/dc1bc23adf1137ef78722176e2da69f8411b1feb/python-package/lightgbm/basic.py#L2298"
  },
  {
    "objectID": "blog/posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "href": "blog/posts/2021-08-18-python-file-not-found-long-file-path-window.html",
    "title": "Python FileNotFoundError or You have a really long file path?",
    "section": "",
    "text": "Solution - Registry\nUpdating your Registry can solve this problem.\n\n\n\n\n\nHello World\n\n\nAfter applying the config, I can finally read the file. :)\n\n\nSummary (TLDR version)\n\nWindow filesystem only allow 256 characters, beyond that you will have trouble to open the file.\nPython will not be able to see this file and throw FileNotFoundError (I have no idea, anyone know why is that?)\nYou can update registry to enable long file path in Window to fix this issue.\n\n(Bonus: Window actually has weird behavior for long filepath, you can try to break it with different ways.)"
  },
  {
    "objectID": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "href": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "",
    "text": "Note\n\n\n\nThese series are written as a quick introduction to software design for data scientists, something that is lightweight than the Design Pattern Bible - Clean Code I wish exists when I first started to learn. Design patterns refer to reusable solutions to some common problems, and some happen to be useful for data science. There is a good chance that someone else has solved your problem before. When used wisely, it helps to reduce the complexity of your code."
  },
  {
    "objectID": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "href": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#so-what-is-callback-after-all",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "So, What is Callback after all?",
    "text": "So, What is Callback after all?\nCallback function, or call after, simply means a function will be called after another function. It is a piece of executable code (function) that passed as an argument to another function. [1]\n\ndef foo(x, callback=None):\n    print('foo!')\n    if callback:\n        callback(x)\n\n\nfoo('123')\n\nfoo!\n\n\n\nfoo('123', print)\n\nfoo!\n123\n\n\nHere I pass the function print as a callback, hence the string 123 get printed after foo!."
  },
  {
    "objectID": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "href": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#why-do-i-need-to-use-callback",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Why do I need to use Callback?",
    "text": "Why do I need to use Callback?\nCallback is very common in high-level deep learning libraries, most likely you will find them in the training loop. * fastai - fastai provide high-level API for PyTorch * Keras - the high-level API for Tensorflow * ignite - they use event & handler, which provides more flexibility in their opinion\n\nimport numpy as np\n\n# A boring training Loop\ndef train(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1  # Pretend we are training the model\n\n\nx = np.ones(10)\ntrain(x);\n\nSo, let’s say you now want to print the loss at the end of an epoch. You can just add 1 lines of code.\n\nThe simple approach\n\ndef train_with_print(x):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n        for batch in range(n_batches):\n            loss = loss - 1 # Pretend we are training the model\n        print(f'End of Epoch. Epoch: {epoch}, Loss: {loss}')\n    return loss\n\n\ntrain_with_print(x);\n\nEnd of Epoch. Epoch: 0, Loss: 18\nEnd of Epoch. Epoch: 1, Loss: 16\nEnd of Epoch. Epoch: 2, Loss: 14\n\n\n\n\nCallback approach\nOr you call add a PrintCallback, which does the same thing but with a bit more code.\n\nclass Callback:\n    def on_epoch_start(self, x):\n        pass\n\n    def on_epoch_end(self, x):\n        pass\n\n    def on_batch_start(self, x):\n        pass\n\n    def on_batch_end(self, x):\n        pass\n\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'End of Epoch. Loss: {x}')\n\n\ndef train_with_callback(x, callback=None):\n    n_epochs = 3\n    n_batches = 2\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callback.on_epoch_start(loss)\n\n        for batch in range(n_batches):\n            callback.on_batch_start(loss)\n            loss = loss - 1  # Pretend we are training the model\n            callback.on_batch_end(loss)\n\n        callback.on_epoch_end(loss)\n\n\ntrain_with_callback(x, callback=PrintCallback());\n\nEnd of Epoch. Loss: 18\nEnd of Epoch. Loss: 16\nEnd of Epoch. Loss: 14\n\n\nUsually, a callback defines a few particular events on_xxx_xxx, which indicate that the function will be executed according to the corresponding condition. So all callbacks will inherit the base class Callback, and override the desired function, here we only implemented the on_epoch_end method because we only want to show the loss at the end.\nIt may seem awkward to write so many more code to do one simple thing, but there are good reasons. Consider now you need to add more features, how would you do it?\n\nModelCheckpoint\nEarly Stopping\nLearningRateScheduler\n\nYou can just add code in the loop, but it will start growing into a really giant function. It is impossible to test this function because it does 10 things at the same time. In addition, the extra code may not even be related to the training logic, they are just there to save the model or plot a chart. So, it is best to separate the logic. A function should only do 1 thing according to the Single Responsibility Principle. It helps you to reduce the complexity as it provides a nice abstraction, you are only modifying code within the specific callback you are interested.\n\n\nAdd some more sauce!\nWhen using the Callback Pattern, I can just implement a few more classes and the training loop is barely touched. Here we introduce a new class Callbacks because we need to execute more than 1 callback, it is used for holding all callbacks and executed them sequentially.\n\nclass Callbacks:\n    \"\"\"\n    It is the container for callback\n    \"\"\"\n\n    def __init__(self, callbacks):\n        self.callbacks = callbacks\n\n    def on_epoch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_start(x)\n\n    def on_epoch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_epoch_end(x)\n\n    def on_batch_start(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_start(x)\n\n    def on_batch_end(self, x):\n        for callback in self.callbacks:\n            callback.on_batch_end(x)\n\nThen we implement the new Callback one by one, here we only have the pseudocode, but you should get the gist. For example, we only need to save the model at the end of an epoch, thus we implement the method on_epoch_end with a ModelCheckPoint callback.\n\nclass PrintCallback(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: End of Epoch. Loss: {x}')\n\n\nclass ModelCheckPoint(Callback):\n    def on_epoch_end(self, x):\n        print(f'[{type(self).__name__}]: Save Model')\n\n\nclass EarlyStoppingCallback(Callback):\n    def on_epoch_end(self, x):\n        if x < 16:\n            print(f'[{type(self).__name__}]: Early Stopped')\n\n\nclass LearningRateScheduler(Callback):\n    def on_batch_end(self, x):\n        print(f'    [{type(self).__name__}]: Reduce learning rate')\n\nAnd we also modify the training loop a bit, the argument now takes a Callbacks which contain zero to many callbacks.\n\ndef train_with_callbacks(x, callbacks=None):\n    n_epochs = 2\n    n_batches = 3\n    loss = 20\n\n    for epoch in range(n_epochs):\n\n        callbacks.on_epoch_start(loss)                             # on_epoch_start\n        for batch in range(n_batches):\n            callbacks.on_batch_start(loss)                         # on_batch_start\n            loss = loss - 1  # Pretend we are training the model\n            callbacks.on_batch_end(loss)                           # on_batch_end\n        callbacks.on_epoch_end(loss)                               # on_epoch_end\n\n\ncallbacks = Callbacks([PrintCallback(), ModelCheckPoint(),\n                      EarlyStoppingCallback(), LearningRateScheduler()])\ntrain_with_callbacks(x, callbacks=callbacks)\n\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 17\n[ModelCheckPoint]: Save Model\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n    [LearningRateScheduler]: Reduce learning rate\n[PrintCallback]: End of Epoch. Loss: 14\n[ModelCheckPoint]: Save Model\n[EarlyStoppingCallback]: Early Stopped\n\n\nHopefully, it convinces you Callback makes the code cleaner and easier to maintain. If you just use plain if-else statements, you may end up with a big chunk of if-else clauses.\n\nfastai - fastai provide high-level API for PyTorch\nKeras - the high-level API for Tensorflow\nignite - they use event & handler, which provides more flexibility in their opinion"
  },
  {
    "objectID": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "href": "blog/posts/2021-07-10-5minutes-data-science-design-pattern-callback.html#reference",
    "title": "5 Minutes Data Science Design Patterns I - Callback",
    "section": "Reference",
    "text": "Reference\n\nhttps://stackoverflow.com/questions/824234/what-is-a-callback-function"
  },
  {
    "objectID": "blog/posts/2021-03-27-microsoft-azure-dp100.html",
    "href": "blog/posts/2021-03-27-microsoft-azure-dp100.html",
    "title": "Microsoft Azure - DP100",
    "section": "",
    "text": "Last Updated: 2021-04-22"
  },
  {
    "objectID": "blog/posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "href": "blog/posts/2021-03-27-microsoft-azure-dp100.html#official-suggested-materials",
    "title": "Microsoft Azure - DP100",
    "section": "Official Suggested Materials",
    "text": "Official Suggested Materials\n\n❗ https://docs.microsoft.com/en-us/learn/paths/build-ai-solutions-with-azure-ml-service/ - This should be your main focus, try to finish the labs and read the tutorials. You need to understand the use case of different products and familiar yourself with the syntax of Azure ML SDK etc.\nhttps://docs.microsoft.com/en-us/learn/paths/create-no-code-predictive-models-azure-machine-learning/ - You should at least finish 1 of the lab to get some sense of the UI. It would be included in the exam for sure (2-3 questions maybe)\nhttps://docs.microsoft.com/en-us/learn/paths/create-machine-learn-models/ - I didn’t spend much time on it as most of them are baisc data science concepts. You would need to how to apply different types of models (Regression/Classification/Time Series) & AutoML for given scenario."
  },
  {
    "objectID": "blog/posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "href": "blog/posts/2021-03-27-microsoft-azure-dp100.html#compute-target",
    "title": "Microsoft Azure - DP100",
    "section": "Compute Target",
    "text": "Compute Target\nMachine Learning Studio - single/multi - Development/Experiment - Local Machine/Cloud VM. - Scale up to larger data/distributed - training compute target - Azure Machine Learning compute cluster - Azure Machine Learning compute instance - Deploy Compute Target (You need to able to judge the most appropiate option based on the context.) - Local web service - Azure Kubernetes Service (AKS) - Azure Container Instances - Azure Machine Learning compute clusters (Batch Inference)"
  },
  {
    "objectID": "blog/posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "href": "blog/posts/2021-03-27-microsoft-azure-dp100.html#datastore",
    "title": "Microsoft Azure - DP100",
    "section": "DataStore",
    "text": "DataStore\n\nAzure Storage (blob and file containers)\nAzure Data Lake stores\nAzure SQL Database\nAzure Databricks file system (DBFS)"
  },
  {
    "objectID": "blog/posts/2020-12-10-typer-create-command-line-and-use-it-anywhere.html",
    "href": "blog/posts/2020-12-10-typer-create-command-line-and-use-it-anywhere.html",
    "title": "Create python command line in few lines, and use it anywhere as a standalone tool!",
    "section": "",
    "text": "import typer\n\n\ndef main(name: str):\n    typer.echo(f\"Hello {name}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n\nOverwriting main1.py\n\n\n\n!python main1.py world\n\nHello world\n\n\n\n!python main1.py --help\n\nUsage: main1.py [OPTIONS] NAME\n\nArguments:\n  NAME  [required]\n\nOptions:\n  --install-completion [bash|zsh|fish|powershell|pwsh]\n                                  Install completion for the specified shell.\n  --show-completion [bash|zsh|fish|powershell|pwsh]\n                                  Show completion for the specified shell, to\n                                  copy it or customize the installation.\n\n  --help                          Show this message and exit.\n\n\nHere I write a new file to main1.py and execute it as a command line with just 5 lines of code. It always comes with a help message for free. Let’s see another example.\n\nimport typer\n\n\ndef main(name: str, age: int = 20, height_meters: float = 1.89, female: bool = True):\n    typer.echo(f\"NAME is {name}, of type: {type(name)}\")\n    typer.echo(f\"--age is {age}, of type: {type(age)}\")\n    typer.echo(f\"--height-meters is {height_meters}, of type: {type(height_meters)}\")\n    typer.echo(f\"--female is {female}, of type: {type(female)}\")\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n\nWriting main2.py\n\n\n\n!python main2.py --help\n\nUsage: main2.py [OPTIONS] NAME\n\nArguments:\n  NAME  [required]\n\nOptions:\n  --age INTEGER                   [default: 20]\n  --height-meters FLOAT           [default: 1.89]\n  --female / --no-female          [default: True]\n  --install-completion [bash|zsh|fish|powershell|pwsh]\n                                  Install completion for the specified shell.\n  --show-completion [bash|zsh|fish|powershell|pwsh]\n                                  Show completion for the specified shell, to\n                                  copy it or customize the installation.\n\n  --help                          Show this message and exit.\n\n\nThis time, we can see that the help message even supplement the expected datatype. Typer will validate the type and conevrt it when possible.\n\n!python main2.py Nok --age=3\n\nNAME is Nok, of type: <class 'str'>\n--age is 3, of type: <class 'int'>\n--height-meters is 1.89, of type: <class 'float'>\n--female is True, of type: <class 'bool'>\n\n\nThe command line works file, but it only works in the same directory, and you always have to type the keyword python. With python setuptools, we can actually installed a command line and run it anywhere. It is pretty easy with just 1 trick, let’s go back to the simple Hello command.\n\nimport typer\n\ndef hello(name:str):\n    typer.echo(f\"Hello {name}\")\n    \ndef main():\n    typer.run(hello)\n\nOverwriting main3.py\n\n\nHere we made a few changes. 1. The logic is move to a new function named hello 2. We removed the __main__ part, as we will not call this python file directly anymore. 3. typer.run(main) is changed to typer.run(hello) and moved inside the main function.\n\nConsole Script\nWe will use setuptool to build console script, which may call the function main. The magic is using console script to install a command line interface (It creates a .exe file) that can be run anywhere. We can name our command line instead of using the filename with a pattern of command_name=file:func_name. Here our function main is inside a file main3.py, so we use hello=main3:main.\n\nfrom setuptools import setup, find_packages\n\n\nsetup(\n    name=\"my_library\",\n    version=\"1.0\",\n    packages=find_packages(),\n        entry_points = {\n        'console_scripts': ['hello=main3:main']}\n)\n\nOverwriting setup.py\n\n\nThen we install the console script .\n\n!python setup.py develop\n\n\n!hello world\n\nHello world\n\n\nWe can now call hello anywhere, as it is installed as a executable.\n\n!where hello\n\nC:\\ProgramData\\Miniconda3\\Scripts\\hello.exe\n\n\nIt’s time for you to build your own commands. This can be easily extended to support multiple commands. https://github.com/tiangolo/typer"
  },
  {
    "objectID": "blog/posts/2020-04-10-presentation-ready-chart.html",
    "href": "blog/posts/2020-04-10-presentation-ready-chart.html",
    "title": "Making Powerpoint Ready Chart with matplotlib",
    "section": "",
    "text": "Large Font Size Title\nIn reality, you probably don’t need a title as big as this one. But using library defautls often is not the best choice."
  },
  {
    "objectID": "blog/posts/2020-04-10-presentation-ready-chart.html#avoid-low-resolution-chart",
    "href": "blog/posts/2020-04-10-presentation-ready-chart.html#avoid-low-resolution-chart",
    "title": "Making Powerpoint Ready Chart with matplotlib",
    "section": "Avoid Low Resolution Chart",
    "text": "Avoid Low Resolution Chart\n\n\n\n\n\n\nNote\n\n\n\nBelieve it or not, a low resolution chart looks much less conviencing. Taking screenshot with larger charts helps you to preserve the resolution.\n\n\n\nResolution of the chart is much better\nMore obvious Title & Label (Try take a few step back from your monitor, see if you can read it)"
  },
  {
    "objectID": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html",
    "href": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html",
    "title": "Hong Kong Elevation map with rayshader (with R)",
    "section": "",
    "text": "This blog is mainly reproducing the blog with different data https://www.tylermw.com/a-step-by-step-guide-to-making-3d-maps-with-satellite-imagery-in-r/. My impression is that R is doing so much better for graph compare to Python. (ggplot and now rayshader for 3D plots!)"
  },
  {
    "objectID": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#data",
    "href": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#data",
    "title": "Hong Kong Elevation map with rayshader (with R)",
    "section": "Data",
    "text": "Data\nTwo datasets was used for this images. Landset for RGB * LC08_L1TP_122044_20200218_20200225_01_T1.TIF\nSRTM 30M resolution elevation map * n21e113.hgt * n21e114.hgt * n22e113.hgt * n22e114.hgt The USGS explorer is a very nice tool to search data.\nI actually couldn’t find a Landsat image cover entire hong kong (some western part is missing). Further enhancement is needed for stitching together different images."
  },
  {
    "objectID": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#setup",
    "href": "blog/posts/2020-11-14-hong-kong-elevation-map-with-rayshader.html#setup",
    "title": "Hong Kong Elevation map with rayshader (with R)",
    "section": "Setup",
    "text": "Setup\n\nconda with R Kernel\nJupyter Notebook\nfastpages\nrayshader\n\n\nUse conda install even for R Packages, I spend hours to get the environment going back and forth in Windows and Linux\n\n\n## Library\nlibrary(rayshader)\nlibrary(sp)\nlibrary(raster)\nlibrary(scales)\nlibrary(dplyr)\n\n\nelevation1 = raster::raster(\"../data/rayshader/HongKong/N21E113.hgt\")\nelevation2 = raster::raster(\"../data/rayshader/HongKong/N21E114.hgt\")\nelevation3 = raster::raster(\"../data/rayshader/HongKong/N22E113.hgt\")\nelevation4 = raster::raster(\"../data/rayshader/HongKong/N22E114.hgt\")\n\nLet’s plot the elevation map. The whole image is green-ish because most of the area is ocean, so they are at sea-level. The orange color indicate a higher elevation.\n\nhk_elevation = raster::merge(elevation1,elevation2, elevation3, elevation4)\n\nheight_shade(raster_to_matrix(hk_elevation)) %>%\nplot_map();\n\n\n\n\nelevation\n\n\nNext, we are going to process the RGB image from Landsat-8 ,The raw jpeg look like this.\n\n\n\nraw_jpeg\n\n\nSatellite raw images requries some preprocessing, before they look like what we expected.\n\nhk_r = raster::raster(\"../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B4.TIF\")\nhk_g = raster::raster(\"../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B3.TIF\")\nhk_b = raster::raster(\"../data/rayshader/HongKong/LC08_L1TP_122044_20200218_20200225_01_T1_B2.TIF\")\n\n\nhk_rbg_corrected = sqrt(raster::stack(hk_r, hk_g, hk_b))\nraster::plotRGB(hk_rbg_corrected);\n\n\n\n\nraw_corrected\n\n\nThe image is quite hazzy, which doesn’t look like the jpeg we saw earlier. We need to improve the contrast.\n\n# Since the RGB image and elevation map does not use the same coordinate system, we need to do some projections.\nhk_elevation_utm = raster::projectRaster(hk_elevation, crs = crs(hk_r), method = \"bilinear\")\ncrs(hk_elevation_utm)\n\nbottom_left = c(y=113.888, x=22.1365)\ntop_right   = c(y=114.330, x=22.5493)\n\nextent_latlong = sp::SpatialPoints(rbind(bottom_left, top_right), proj4string=sp::CRS(\"+proj=longlat +ellps=WGS84 +datum=WGS84\"))\nextent_utm = sp::spTransform(extent_latlong, raster::crs(hk_elevation_utm))\n\ne = raster::extent(extent_utm)\ne\n\nCRS arguments:\n +proj=utm +zone=49 +datum=WGS84 +units=m +no_defs \n\n\nclass      : Extent \nxmin       : 797906.6 \nxmax       : 842523 \nymin       : 2450766 \nymax       : 2497449 \n\n\n\nhk_rgb_cropped = raster::crop(hk_rbg_corrected, e)\nelevation_cropped = raster::crop(hk_elevation_utm, e)\n\nnames(hk_rgb_cropped) = c(\"r\",\"g\",\"b\")\n\nhk_r_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$r)\nhk_g_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$g)\nhk_b_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$b)\n\nhkel_matrix = rayshader::raster_to_matrix(elevation_cropped)\n\nhk_rgb_array = array(0,dim=c(nrow(hk_r_cropped),ncol(hk_r_cropped),3))\n\nhk_rgb_array[,,1] = hk_r_cropped/255 #Red layer\nhk_rgb_array[,,2] = hk_g_cropped/255 #Blue layer\nhk_rgb_array[,,3] = hk_b_cropped/255 #Green layer\n\nhk_rgb_array = aperm(hk_rgb_array, c(2,1,3))\n\nplot_map(hk_rgb_array)\n\n\n\n\nhazzy\n\n\nThe whole image is bright because we have some dark pixels in the corner. It’s similiar to taking images in a dark room, any light source will become a bright spot.\nWe can improve this by stretching the intensity. It’s really no different than how you fine tune your images on Instagram.\n\nhk_rgb_cropped = raster::crop(hk_rbg_corrected, e)\nelevation_cropped = raster::crop(hk_elevation_utm, e)\n# Stretch the images\nhk_rgb_cropped <-\nraster::stretch(hk_rgb_cropped,\nminq = .01,\nmaxq = .999,\n)\n\nnames(hk_rgb_cropped) = c(\"r\",\"g\",\"b\")\n\nhk_r_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$r)\nhk_g_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$g)\nhk_b_cropped = rayshader::raster_to_matrix(hk_rgb_cropped$b)\n\nhkel_matrix = rayshader::raster_to_matrix(elevation_cropped)\n\nhk_rgb_array = array(0,dim=c(nrow(hk_r_cropped),ncol(hk_r_cropped),3))\n\nhk_rgb_array[,,1] = hk_r_cropped/255 #Red layer\nhk_rgb_array[,,2] = hk_g_cropped/255 #Blue layer\nhk_rgb_array[,,3] = hk_b_cropped/255 #Green layer\n\nhk_rgb_array = aperm(hk_rgb_array, c(2,1,3))\nhk_rgb_contrast = scales::rescale(hk_rgb_array,to=c(0,1))\nplot_map(hk_rgb_contrast)\n\n\n\n\nbright\n\n\nNow we get a much better image\n\nplot_3d(hk_rgb_contrast, hkel_matrix, windowsize = c(1100,900), zscale = 15, shadowdepth = -50,\n        zoom=0.5, phi=45,theta=-15,fov=70, background = \"#F2E1D0\", shadowcolor = \"#523E2B\")\n\n\nrender_scalebar(limits=c(0, 5, 10),label_unit = \"km\",position = \"W\", y=50,\n                scale_length = c(0.33,1))\nrender_compass(position = \"N\")\nrender_snapshot(title_text = \"Hong Kong | Imagery: Landsat 8 | DEM: 30m SRTM\",\n                title_bar_color = \"#000000\", title_color = \"white\", title_bar_alpha = 1,\n               clear=TRUE, )\n\n\n\n\n3d"
  },
  {
    "objectID": "blog/posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "href": "blog/posts/2021-05-29-python-internal-series-python-gil-and-memory.html",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "",
    "text": "Most people first learn about GIL because of how it slows down Python program and prevent multi-threading running efficiently, however, the GIL is one of the reason why Python survive 30 years and still growing healthyly.\nGIL is nothing like the stereotype people think, legacy, slow. There are multiple benefits GIL provide:\n\nIt speed ups single thread program.\nIt is compatible with many C Program thanks to the C API of CPysthon."
  },
  {
    "objectID": "blog/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "href": "blog/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#global-interpreter-lock-a.k.a-mutex-lock",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Global Interpreter Lock a.k.a Mutex Lock",
    "text": "Global Interpreter Lock a.k.a Mutex Lock\nTo start with, GIL is a mutex lock."
  },
  {
    "objectID": "blog/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "href": "blog/posts/2021-05-29-python-internal-series-python-gil-and-memory.html#why-gil-is-needed-in-the-first-place",
    "title": "Python Internal Series - Global Interpreter Lock (GIL) and Memory Management",
    "section": "Why GIL is needed in the first place?",
    "text": "Why GIL is needed in the first place?\nMemory management. Python use something called “reference counting”, which make it different from many modern programming lanaguage. It is what allow Python programmer to lay back and let Python take care when to release memory. Precisely, it is actually the C program controlling the memory life cycle for Python (Cpython). Cpython is known as the default Python interpreter. It first compiles Python to intermediate bytecode (.pyc files). These bytecode then being interpreted by a virtual machine ane executed. It is worth to mention that other variants of Python exist, i.e. IronPython(C#), Jython(Java), Pypy(Python) and they have different memory management mechanisms.\n\nPython Memory Management - Reference Count & Garbage Collection (gc)\n\nimport sys\n\n\nsys.getrefcount(a)\n\n3\n\n\nReference counting is a simple idea. The intuition is that if a particular object is not referenced by anything, it can be recycled since it will not be used anymore.\nFor example, the list [1] is now referenced by the variable a, so the reference count is incremented by 1.\n\nimport sys\na = [1]\nsys.getrefcount(a)\n\n2\n\n\nNote that the reference count is 2 instead of 1. 1. The first reference is a = [1] 2. When the variable a is passed to sys.getrefcount(a) as an argument, it also increases the reference count.\n\ndel a\n\nWhen del a is called, the list [1] have 0 reference count, and it is collected by Python automatically behind the scene.\n\n\nLock & Deadlock\n\n\nMemory Management"
  },
  {
    "objectID": "kedro/index.html",
    "href": "kedro/index.html",
    "title": "Kedro’s related stuff",
    "section": "",
    "text": "Fill in a module description here\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nkedro\n\n\n\n\nKedro pipeline offers some nice feature like automatically release data in memory that is no longer need. How is this possible? Let’s dive deep into the code.\n\n\n\n\n\n\nJul 2, 2021\n\n\nnoklam\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kedro/posts/2022-10-11-kedro-debug-runner.html",
    "href": "kedro/posts/2022-10-11-kedro-debug-runner.html",
    "title": "core",
    "section": "",
    "text": "DebugRunner\n\n DebugRunner (is_async:bool=False)\n\nSequentialRunner is an AbstractRunner implementation. It can be used to run the Pipeline in a sequential manner using a topological sort of provided nodes.\n\n\n\nDebugRunner\n\n DebugRunner (is_async:bool=False)\n\nSequentialRunner is an AbstractRunner implementation. It can be used to run the Pipeline in a sequential manner using a topological sort of provided nodes.\n\n# `DebugRunner` has to be used in a different way since `session.run` don't support additional argument, so we are going to use a lower level approach and construct `Runner` and `Pipeline` and `DataCatalog` ourselves.\n\n# Testing Kedro Project: https://github.com/noklam/kedro_gallery/tree/master/kedro-debug-runner-demo\n\nThe kedro.ipython extension is already loaded. To reload it, use:\n  %reload_ext kedro.ipython\n[10/06/22 14:45:20] INFO     Updated path to Kedro project:       __init__.py:54\n                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n                             y/kedro-debug-runner-demo                          \n[10/06/22 14:45:22] INFO     Kedro project                        __init__.py:77\n                             kedro_debug_runner_demo                            \n                    INFO     Defined global variable 'context',   __init__.py:78\n                             'session', 'catalog' and 'pipelines'               \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_1 = runner.run(default_pipeline, catalog)\n\n                    INFO     Updated path to Kedro project:       __init__.py:54\n                             /Users/Nok_Lam_Chan/dev/kedro_galler               \n                             y/kedro-debug-runner-demo                          \n[10/06/22 14:45:24] INFO     Kedro project                        __init__.py:77\n                             kedro_debug_runner_demo                            \n                    INFO     Defined global variable 'context',   __init__.py:78\n                             'session', 'catalog' and 'pipelines'               \n                    INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -> [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -> [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) ->                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_2 = runner.run(default_pipeline, catalog, dataset_names=[\"example_iris_data\"])\n\n[10/06/22 14:45:27] INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -> [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -> [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) ->                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n                    INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n\n\n\nrunner = DebugRunner()\ndefault_pipeline = pipelines[\"__default__\"]\nrun_3 = runner.run(default_pipeline, catalog, dataset_names=[\"X_train\"]) # Input datasets\n\n[10/06/22 14:46:01] INFO     Loading data from               data_catalog.py:343\n                             'example_iris_data'                                \n                             (CSVDataSet)...                                    \n                    INFO     Loading data from 'parameters'  data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: split:                    node.py:327\n                             split_data([example_iris_data,parameter            \n                             s]) -> [X_train,X_test,y_train,y_test]             \n                    INFO     Saving data to 'X_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'X_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_train'        data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Saving data to 'y_test'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'X_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: make_predictions:         node.py:327\n                             make_predictions([X_train,X_test,y_trai            \n                             n]) -> [y_pred]                                    \n                    INFO     Saving data to 'y_pred'         data_catalog.py:382\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_pred'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Loading data from 'y_test'      data_catalog.py:343\n                             (MemoryDataSet)...                                 \n                    INFO     Running node: report_accuracy:          node.py:327\n                             report_accuracy([y_pred,y_test]) ->                \n                             None                                               \n                    INFO     Model has accuracy of 0.933 on test     nodes.py:74\n                             data.                                              \n                    INFO     Loading data from 'X_train'     data_catalog.py:343\n                             (MemoryDataSet)...                                 \n\n\n\nrun_1\n\n{}\n\n\n\nrun_2\n\n{'example_iris_data':      sepal_length  sepal_width  petal_length  petal_width    species\n 0             5.1          3.5           1.4          0.2     setosa\n 1             4.9          3.0           1.4          0.2     setosa\n 2             4.7          3.2           1.3          0.2     setosa\n 3             4.6          3.1           1.5          0.2     setosa\n 4             5.0          3.6           1.4          0.2     setosa\n ..            ...          ...           ...          ...        ...\n 145           6.7          3.0           5.2          2.3  virginica\n 146           6.3          2.5           5.0          1.9  virginica\n 147           6.5          3.0           5.2          2.0  virginica\n 148           6.2          3.4           5.4          2.3  virginica\n 149           5.9          3.0           5.1          1.8  virginica\n \n [150 rows x 5 columns]}\n\n\n\nrun_3\n\n{'X_train':      sepal_length  sepal_width  petal_length  petal_width\n 47            4.6          3.2           1.4          0.2\n 3             4.6          3.1           1.5          0.2\n 31            5.4          3.4           1.5          0.4\n 25            5.0          3.0           1.6          0.2\n 15            5.7          4.4           1.5          0.4\n ..            ...          ...           ...          ...\n 28            5.2          3.4           1.4          0.2\n 78            6.0          2.9           4.5          1.5\n 146           6.3          2.5           5.0          1.9\n 49            5.0          3.3           1.4          0.2\n 94            5.6          2.7           4.2          1.3\n \n [120 rows x 4 columns]}\n\n\n\n\n\nGreedySequentialRunner\n\n GreedySequentialRunner (is_async:bool=False)\n\nSequentialRunner is an AbstractRunner implementation. It can be used to run the Pipeline in a sequential manner using a topological sort of provided nodes.\n\n\n\nGreedySequentialRunner\n\n GreedySequentialRunner (is_async:bool=False)\n\nSequentialRunner is an AbstractRunner implementation. It can be used to run the Pipeline in a sequential manner using a topological sort of provided nodes."
  },
  {
    "objectID": "kedro/posts/2021-07-02-kedro-datacatalog.html",
    "href": "kedro/posts/2021-07-02-kedro-datacatalog.html",
    "title": "Advance Kedro Series - Digging into Dataset Memory Management and CacheDataSet",
    "section": "",
    "text": "This article assumes you have basic understanding of kedro, I will focus on CacheDataSet and the auto-release dataset feature of kedro pipeline. It is useful to reduce your memory footprint without encountering the infamous Out of Memory (OOM) issue.\nTo start with, we have the default iris dataset. Normally we would do it in a YAML file, but to make things easier in Notebook, I’ll keep everything compact in a notebook.\n\nimport kedro\nkedro.__version__\n\n'0.17.4'\n\n\n\nfrom kedro.io import DataCatalog, MemoryDataSet, CachedDataSet\nfrom kedro.extras.datasets.pandas import CSVDataSet\nfrom kedro.pipeline import node, Pipeline\nfrom kedro.runner import SequentialRunner\n\n# Prepare a data catalog\ndata_catalog = DataCatalog({\"iris\": CSVDataSet('data/01_raw/iris.csv')})\n\nNext, we have a pipeline follows this execution order: A -> B -> C\n\nfrom kedro.pipeline import Pipeline, node\nimport pandas as pd\n\n\ndef A(df):\n    print('Loading the Iris Dataset')\n    return 'Step1'\n\n\ndef B(dummy):\n    return 'Step2'\n\n\ndef C(dummy):\n    return 'Step3'\n\n\npipeline = Pipeline([node(A, \"iris\", \"A\"),\n                     node(B, \"A\", \"B\"),\n                     node(C, \"B\", \"C\"),\n                    ])\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTo zoom in to the pipeline, we can use Hook to print out the catalog after every node’s run.\n\nfrom kedro.framework.hooks import hook_impl\nfrom kedro.framework.hooks import get_hook_manager\nfrom pprint import pprint\n\ndef apply_dict(d):\n    new_dict = {}\n    for k, v in d.items():\n        if isinstance(v, CachedDataSet):\n            if v._cache.exists():\n                print(v._cache._data)\n                new_dict[k] = 'In Memory'\n            else:\n                new_dict[k] ='Cache Deleted'\n        elif v.exists():\n            new_dict[k] = 'In Memory'\n    return new_dict\n\n\nclass DebugHook:\n    \"\"\"A hook class for creating a post mortem debugging with the PDB debugger\n    whenever an error is triggered within a pipeline. The local scope from when the\n    exception occured is available within this debugging session.\n    \"\"\"\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        pprint(f\"Print Catalog {apply_dict(catalog._data_sets)}\")\n#         pprint(f\"Print Catalog {apply_dict2(lambda x:x.exists(), catalog._data_sets)}\")\n        print(\"*****************************\")\n        \nhook_manager = get_hook_manager()\ndebug_hook = hook_manager.register(DebugHook());\n\nThis hook will print out dataset that exist in data catalog. It is a bit tricky because kedro did not delete the dataset, it marked the underlying data as _EMPTY object instead.\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -> [B]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -> [C]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\nLet’s have a look at what happened when a SequentialRunner runs a pipeline.\nIt is interesting to note that kedro takes a similar approach to Python, it uses reference counting to control the dataset life cycle. If you are interested, I have another post to dive into Python Memory Management.\n            # decrement load counts and release any data sets we've finished with\n            for data_set in node.inputs:\n                load_counts[data_set] -= 1\n                if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                    catalog.release(data_set)\n            for data_set in node.outputs:\n                if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                    catalog.release(data_set)\n\nCacheDataSet\nWhat does release do? It will remove the underlying data if this data is stored in memory.\n# In CSVDataSet\nhttps://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/pandas/csv_dataset.py#L176-L178\n```python\ndef _release(self) -> None:\n    super()._release()\n    self._invalidate_cache()\n# In CacheDataSet\ndef _release(self) -> None:\n    self._cache.release()\n    self._dataset.release()\n# In MemoryDataSet\ndef _release(self) -> None:\n    self._data = _EMPTY\nFirst, we can test if it works as expected.\n\nd = CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))\nd.load()\nd._cache._data.head()\n\n\n\n\n\n  \n    \n      \n      sepal_length\n      sepal_width\n      petal_length\n      petal_width\n      species\n    \n  \n  \n    \n      0\n      5.1\n      3.5\n      1.4\n      0.2\n      setosa\n    \n    \n      1\n      4.9\n      3.0\n      1.4\n      0.2\n      setosa\n    \n    \n      2\n      4.7\n      3.2\n      1.3\n      0.2\n      setosa\n    \n    \n      3\n      4.6\n      3.1\n      1.5\n      0.2\n      setosa\n    \n    \n      4\n      5.0\n      3.6\n      1.4\n      0.2\n      setosa\n    \n  \n\n\n\n\n\nd.exists()\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nTrue\n\n\n\nd.release()\n\n\nd._cache.exists()\n\nFalse\n\n\nThis is the expected behavior, where the cache should be released. However, it seems not to be the case when I run the pipeline.\n\ndata_catalog = DataCatalog({\"iris\": CachedDataSet(CSVDataSet('data/01_raw/iris.csv'))})\nrunner.run(pipeline, data_catalog)\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory'}\"\n*****************************\nFinish node B([A]) -> [B]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'A': 'In Memory'}\"\n*****************************\nFinish node C([B]) -> [C]\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\"Print Catalog {'iris': 'In Memory', 'B': 'In Memory'}\"\n*****************************\n\n\n{'C': 'Step3'}\n\n\nThe dataset is persisted throughout the entire pipeline, why? We can monkey patch the SequentialRunner to check why is this happening.\n\n\nA potential bug or undesired beahvior?\n\nfrom collections import Counter\nfrom itertools import chain\nfrom kedro.runner.runner import AbstractRunner, run_node\n\ndef _run(\n    self, pipeline, catalog, run_id = None\n) -> None:\n    \"\"\"The method implementing sequential pipeline running.\n\n    Args:\n        pipeline: The ``Pipeline`` to run.\n        catalog: The ``DataCatalog`` from which to fetch data.\n        run_id: The id of the run.\n\n    Raises:\n        Exception: in case of any downstream node failure.\n    \"\"\"\n    nodes = pipeline.nodes\n    done_nodes = set()\n\n    load_counts = Counter(chain.from_iterable(n.inputs for n in nodes))\n\n    for exec_index, node in enumerate(nodes):\n        try:\n            run_node(node, catalog, self._is_async, run_id)\n            done_nodes.add(node)\n        except Exception:\n            self._suggest_resume_scenario(pipeline, done_nodes)\n            raise\n            \n        # print load counts for every node run\n        pprint(f\"{load_counts}\")\n        print(\"pipeline input: \", pipeline.inputs())\n        print(\"pipeline output: \", pipeline.outputs())\n\n        # decrement load counts and release any data sets we've finished with\n        for data_set in node.inputs:\n            load_counts[data_set] -= 1\n            if load_counts[data_set] < 1 and data_set not in pipeline.inputs():\n                catalog.release(data_set)\n        for data_set in node.outputs:\n            if load_counts[data_set] < 1 and data_set not in pipeline.outputs():\n                catalog.release(data_set)\n\n        self._logger.info(\n            \"Completed %d out of %d tasks\", exec_index + 1, len(nodes)\n        )\n        \nSequentialRunner._run = _run\n\nc:\\programdata\\miniconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n\n\nNow we re-run the pipeline. Let’s reset the hook to only print related information.\n\nclass PrintHook:\n    @hook_impl\n    def after_node_run(self, node, catalog):\n        # adding extra behaviour to a single node\n        print(f\"Finish node {node.name}\")\n        print(\"*****************************\")\n        \n\nhook_manager.set_blocked(debug_hook); # I tried hook_manger.unregister(), but it is not working.\nprint_hook = hook_manager.register(PrintHook())\n\n\n# Create a runner to run the pipeline\nrunner = SequentialRunner()\n\n# Run the pipeline\nrunner.run(pipeline, data_catalog);\n\nLoading the Iris Dataset\nFinish node A([iris]) -> [A]\n*****************************\n\"Counter({'iris': 1, 'A': 1, 'B': 1})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node B([A]) -> [B]\n*****************************\n\"Counter({'A': 1, 'B': 1, 'iris': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\nFinish node C([B]) -> [C]\n*****************************\n\"Counter({'B': 1, 'iris': 0, 'A': 0})\"\npipeline input:  {'iris'}\npipeline output:  {'C'}\n\n\n\n\nConclusion\nSo the reason why the iris data is kept becasue it is always in pipeline.inputs(), which I think is not what we wanted."
  }
]